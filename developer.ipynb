{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import oracledb\n",
    "from sqlalchemy import create_engine, text\n",
    "from flask import Flask, render_template\n",
    "import db\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "title = 'data engineer'\n",
    "location = \"San Jose, CA\" \n",
    "#location = \"Chicago, Illinois, United States\"\n",
    "#location = \"San Jose, California, United States\"\n",
    "#location = \"United States\" \n",
    "job_count = 0\n",
    "headers={\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36\"}\n",
    "#url = f'{base_url}?f_E={exp_level}&f_TPR={post_date}&keywords=\"{title}\"&location={location}&start={job_count}'\n",
    "tracking_all_job_ids = db.get_all_job_ids()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCTION DEFINITIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sends a request for one page of results to LinkedIn to get a list of job titles\n",
    "def get_job_results_page(title, location, job_count) -> list:\n",
    "    base_url = \"https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search\"\n",
    "    exp_level = \"2\"  # 2==entry level\n",
    "    post_date = \"r604800\"  # r604800==Past week\n",
    "    url = f'{base_url}?f_E={exp_level}&f_TPR={post_date}&keywords=\"{title}\"&location={location}&start={job_count}'\n",
    "    print(\"job page URL: \", url)\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    all_job_titles = soup.find_all(\"li\")\n",
    "    return all_job_titles\n",
    "\n",
    "# Parses out the job title from one listing\n",
    "def parse_job_id(job):\n",
    "    return job.find(\"div\", {\"class\":\"base-card\"}).get(\"data-entity-urn\").split(\":\")[3]\n",
    "\n",
    "# Get all job ids from one results page\n",
    "def get_job_ids(title, location, job_count):\n",
    "    job_ids = []\n",
    "    all_job_titles = get_job_results_page(title, location, job_count)\n",
    "    if not all_job_titles:  # if no jobs titles in the list\n",
    "        print(\"No job titles found!\")\n",
    "        return None \n",
    "\n",
    "    all_job_ids = list(map(parse_job_id, all_job_titles))\n",
    "    print(\"all job ids: \", all_job_ids)\n",
    "    return all_job_ids\n",
    "    \n",
    "# Format strings to be in a consistent format \n",
    "def string_format(s: str) -> str:\n",
    "    s = s.replace(\" \", \"_\")     \n",
    "    s = s.replace(\"'\", \"\")\n",
    "    s = s.replace('\"', \"\")\n",
    "\n",
    "    return s\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "def get_job_details_linkedin(job_ids):\n",
    "    job_url = 'https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/{}'\n",
    "    l_all_job_info = []\n",
    "\n",
    "    for id in job_ids:\n",
    "        d_job_info = {}\n",
    "        d_job_info[\"job_id\"] = int(id)\n",
    "        d_job_info[\"job_search_term\"] = title\n",
    "        d_job_info[\"location_search_term\"] = location\n",
    "        job_desc_url = job_url.format(id)\n",
    "        print(job_desc_url)\n",
    "        res = requests.get(job_desc_url)\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "        # If the response code is 429, then we need to slow down scraping\n",
    "        if res.status_code == 429:\n",
    "            print(\"Got blocked LinkedIn...Slow down!!\")\n",
    "            time.sleep(120)\n",
    "        else:\n",
    "            # Putting in a 2 second delay for scraping\n",
    "            time.sleep(2)\n",
    "\n",
    "        \n",
    "        # Get the company name\n",
    "        d_job_info[\"company\"] = soup.find(\"div\", {\"class\":\"top-card-layout__card\"}).find(\"a\").find(\"img\").get(\"alt\")\n",
    "\n",
    "        # Get the location\n",
    "        d_job_info[\"location\"] = soup.find(\"div\", {\"class\":\"topcard__flavor-row\"}).find(\"span\", {\"class\":\"topcard__flavor--bullet\"}).text.strip()\n",
    "\n",
    "        # Get the job title\n",
    "        d_job_info[\"job_title\"] = soup.find(\"h2\", {\"class\":\"top-card-layout__title\"}).text.strip()\n",
    "\n",
    "        # Get the full job description \n",
    "        d_job_info[\"job_description\"] = soup.find(\"div\", {\"class\":\"show-more-less-html__markup\"}).get_text(separator=u\"\\n\")\n",
    "        \n",
    "        # Get years of experience!!! Keep checking this as it may be buggy\n",
    "        d_job_info[\"experience\"] = re.findall(r\".*\\D\\d{1,2}\\D.*years?\", d_job_info[\"job_description\"])\n",
    "        d_job_info[\"experience\"] = \"\\n\".join(d_job_info[\"experience\"])\n",
    "        \n",
    "        # Get the max years experience\n",
    "        if d_job_info[\"experience\"]:\n",
    "            d_job_info['max_exp'] = max(re.findall(r'\\d{1,2}', d_job_info[\"experience\"]))\n",
    "        \n",
    "\n",
    "        # Get Seniority level, Employment type, Job function, Industries\n",
    "        job_criteria_list = soup.find(\"ul\", {\"class\":\"description__job-criteria-list\"}).find_all(\"li\")\n",
    "        for criteria in job_criteria_list:\n",
    "            criteria = criteria.text.split(\"\\n\") # convert lines to a list\n",
    "            criteria = [i.strip() for i in criteria if i.strip()] # remove lines with only white space\n",
    "            criteria[0] = criteria[0].replace(\" \", \"_\").lower()\n",
    "            d_job_info.update({criteria[0]:criteria[1]})\n",
    "\n",
    "        # Get job posting date\n",
    "        posting_date = soup.find(\"span\", {\"class\":\"posted-time-ago__text\"}).text.strip()\n",
    "        posting_num = int(re.match(r'\\d{1,2}',posting_date).group())\n",
    "        if \"minute\" in posting_date:\n",
    "            d_job_info[\"posting_date\"] = datetime.today() - timedelta(minutes=posting_num)\n",
    "        elif \"hour\" in posting_date:\n",
    "            d_job_info[\"posting_date\"] = datetime.today() - timedelta(hours=posting_num)\n",
    "        elif \"day\" in posting_date:\n",
    "            d_job_info[\"posting_date\"] = datetime.today() - timedelta(days=posting_num)\n",
    "        else:\n",
    "            d_job_info[\"posting_date\"] = \"\"\n",
    "        \n",
    "        \n",
    "        # !!! Get Other useful info with AI !!!\n",
    "\n",
    "        \n",
    "        # Get URL \n",
    "        d_job_info[\"url\"] = soup.find(\"a\", {\"class\":\"topcard__link\"}).get(\"href\")\n",
    "        print(d_job_info[\"url\"])\n",
    "\n",
    "        # Append the job info (dict) to the list of job info\n",
    "        l_all_job_info.append(d_job_info)\n",
    "\n",
    "    # Convert list of all job info dicts to a dataframe\n",
    "    df_all_job_info = pd.DataFrame(l_all_job_info)\n",
    "    df_all_job_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job page URL:  https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?f_E=2&f_TPR=r604800&keywords=\"data engineer\"&location=St Louis, Missouri, United States&start=0\n",
      "all job ids:  ['3711635924', '3708952912', '3714572531']\n",
      "job page URL:  https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?f_E=2&f_TPR=r604800&keywords=\"data engineer\"&location=St Louis, Missouri, United States&start=25\n",
      "No job titles found!\n",
      "['3711635924', '3708952912', '3714572531']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "job_ids = []\n",
    "job_count = 0\n",
    "max_jobs_to_scrape = 300\n",
    "# I can't get a job_count for the search query, \n",
    "# so going to scrape till it can NOT find any more job ids\n",
    "# or I'm going to scrape upto 300 jobs or 12 pages\n",
    "while job_count <= max_jobs_to_scrape:\n",
    "    page_of_ids = get_job_ids(title, location, job_count)\n",
    "    if page_of_ids:\n",
    "        job_ids.extend(page_of_ids)\n",
    "    else:\n",
    "        break\n",
    "    job_count += 25\n",
    "    \n",
    "print(job_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out Job IDs already in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3711635924], dtype=int64)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert string job ids to ints and convert to numpy array\n",
    "job_ids = np.array(list(map(int, job_ids)))\n",
    "# Jobs ids that exist in the database\n",
    "job_ids_for_db = job_ids[np.in1d(job_ids, tracking_all_job_ids)]\n",
    "job_ids_for_db\n",
    "# Filter out job ids already in the database\n",
    "job_ids_for_linkedin = job_ids[~np.in1d(job_ids, tracking_all_job_ids)]\n",
    "job_ids_for_linkedin\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM tbl_jobs WHERE job_id in (3708952912, 3714572531)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_id</th>\n",
       "      <th>company</th>\n",
       "      <th>location</th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_description</th>\n",
       "      <th>experience</th>\n",
       "      <th>seniority_level</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>job_function</th>\n",
       "      <th>industries</th>\n",
       "      <th>posting_date</th>\n",
       "      <th>url</th>\n",
       "      <th>max_exp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3708952912</td>\n",
       "      <td>BDO USA</td>\n",
       "      <td>St Louis, MO</td>\n",
       "      <td>Tax Digital Transformation &amp; Innovation Senior...</td>\n",
       "      <td>\\n\\nJob Description\\nJob Summary:\\nThe Digital...</td>\n",
       "      <td>Bachelor's degree and six (6) or more years o...</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Finance and Accounting/Auditing</td>\n",
       "      <td>Accounting</td>\n",
       "      <td>2023-09-09 04:00:51</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/tax-digital...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3714572531</td>\n",
       "      <td>ClearanceJobs</td>\n",
       "      <td>St Louis, MO</td>\n",
       "      <td>Data Scientist with Security Clearance</td>\n",
       "      <td>\\n        Job Number: R0179261 Data Scientist\\...</td>\n",
       "      <td>The Opportunity: Ever-expanding technology lik...</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>Part-time</td>\n",
       "      <td>Engineering and Information Technology</td>\n",
       "      <td>Defense and Space Manufacturing</td>\n",
       "      <td>2023-09-07 04:00:52</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/data-scient...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       job_id        company      location   \n",
       "0  3708952912        BDO USA  St Louis, MO  \\\n",
       "1  3714572531  ClearanceJobs  St Louis, MO   \n",
       "\n",
       "                                           job_title   \n",
       "0  Tax Digital Transformation & Innovation Senior...  \\\n",
       "1             Data Scientist with Security Clearance   \n",
       "\n",
       "                                     job_description   \n",
       "0  \\n\\nJob Description\\nJob Summary:\\nThe Digital...  \\\n",
       "1  \\n        Job Number: R0179261 Data Scientist\\...   \n",
       "\n",
       "                                          experience seniority_level   \n",
       "0   Bachelor's degree and six (6) or more years o...     Entry level  \\\n",
       "1  The Opportunity: Ever-expanding technology lik...     Entry level   \n",
       "\n",
       "  employment_type                            job_function   \n",
       "0       Full-time         Finance and Accounting/Auditing  \\\n",
       "1       Part-time  Engineering and Information Technology   \n",
       "\n",
       "                        industries        posting_date   \n",
       "0                       Accounting 2023-09-09 04:00:51  \\\n",
       "1  Defense and Space Manufacturing 2023-09-07 04:00:52   \n",
       "\n",
       "                                                 url max_exp  \n",
       "0  https://www.linkedin.com/jobs/view/tax-digital...    None  \n",
       "1  https://www.linkedin.com/jobs/view/data-scient...    None  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "get_job_details_db(job_ids_for_db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(job_ids_for_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all the job IDs that already exist in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3708157029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3714774167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       job_id\n",
       "0  3708157029\n",
       "1  3714774167"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs = '''(description= (retry_count=20)(retry_delay=3)(address=(protocol=tcps)(port=1521)(host=adb.us-sanjose-1.oraclecloud.com))(connect_data=(service_name=ga3e236c6957ba6_oltpdb_high.adb.oraclecloud.com))(security=(ssl_server_dn_match=yes)))'''\n",
    "user=\"appuser\"\n",
    "password=os.environ['ORACLE_PASSWORD_APPUSER']\n",
    "engine = create_engine(\n",
    "    f'oracle+oracledb://{user}:{password}@{cs}'\n",
    "    )\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    df_jobs_exists = pd.read_sql_query(f'SELECT job_id FROM tbl_jobs WHERE job_id IN ({\",\".join(job_ids)})', conn)\n",
    "               \n",
    "df_jobs_exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out all the IDs that exist in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_id</th>\n",
       "      <th>company</th>\n",
       "      <th>location</th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_description</th>\n",
       "      <th>experience</th>\n",
       "      <th>seniority_level</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>job_function</th>\n",
       "      <th>industries</th>\n",
       "      <th>posting_date</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3715560870</td>\n",
       "      <td>TechFetch.com - On Demand Tech Workforce hirin...</td>\n",
       "      <td>San Jose, CA</td>\n",
       "      <td>Remote - Azure Data Engineer with Data Bricks</td>\n",
       "      <td>\\n        \"ALL our jobs are US based and candi...</td>\n",
       "      <td>\"ALL our jobs are US based and candida...</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>Part-time</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>IT Services and IT Consulting</td>\n",
       "      <td>2023-09-09 04:39:38.138934</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/remote-azur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3708150943</td>\n",
       "      <td>Tanisha Systems, Inc</td>\n",
       "      <td>San Jose, CA</td>\n",
       "      <td>Azure Data Engineer with Data Bricks</td>\n",
       "      <td>\\n        ob role: \\nAzure Data Engineer with ...</td>\n",
       "      <td>Required Skills/Qualifications\\n6+ years\\nDesi...</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>Contract</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Human Resources Services</td>\n",
       "      <td>2023-09-08 04:39:38.722594</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/azure-data-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3712851766</td>\n",
       "      <td>SPECTRAFORCE</td>\n",
       "      <td>Cupertino, CA</td>\n",
       "      <td>Video Data Collector</td>\n",
       "      <td>\\n\\nJob Title: Video Data Collector\\nDuration:...</td>\n",
       "      <td></td>\n",
       "      <td>Entry level</td>\n",
       "      <td>Contract</td>\n",
       "      <td>Art/Creative and Writing/Editing</td>\n",
       "      <td>Staffing and Recruiting</td>\n",
       "      <td>2023-09-09 04:39:39.023121</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/video-data-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3707550926</td>\n",
       "      <td>TalentBurst, an Inc 5000 company</td>\n",
       "      <td>Cupertino, CA</td>\n",
       "      <td>Video Data Collector #: 23-15206</td>\n",
       "      <td>\\n\\nJob Description\\n Title: Video Data Collec...</td>\n",
       "      <td></td>\n",
       "      <td>Entry level</td>\n",
       "      <td>Contract</td>\n",
       "      <td>Accounting/Auditing and Finance</td>\n",
       "      <td>Staffing and Recruiting</td>\n",
       "      <td>2023-09-09 04:39:39.328467</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/video-data-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3708148376</td>\n",
       "      <td>Russell Tobin</td>\n",
       "      <td>Cupertino, CA</td>\n",
       "      <td>Process Technician I</td>\n",
       "      <td>\\n\\nTitle: Video Data Collector\\nJob Type: 6 m...</td>\n",
       "      <td></td>\n",
       "      <td>Entry level</td>\n",
       "      <td>Contract</td>\n",
       "      <td>Management and Manufacturing</td>\n",
       "      <td>Staffing and Recruiting</td>\n",
       "      <td>2023-09-08 04:39:39.735684</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/process-tec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3712609923</td>\n",
       "      <td>SPECTRAFORCE</td>\n",
       "      <td>Cupertino, CA</td>\n",
       "      <td>Video Data Collector</td>\n",
       "      <td>\\n\\nJob Title: Video Data Collector\\nDuration:...</td>\n",
       "      <td></td>\n",
       "      <td>Entry level</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Accounting/Auditing and Finance</td>\n",
       "      <td>Staffing and Recruiting</td>\n",
       "      <td>2023-09-07 04:39:40.045584</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/video-data-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3707532523</td>\n",
       "      <td>TalentBurst, an Inc 5000 company</td>\n",
       "      <td>Cupertino, CA</td>\n",
       "      <td>Video Data Collector</td>\n",
       "      <td>\\n\\nTitle: Video Data Collector\\n \\n Location:...</td>\n",
       "      <td></td>\n",
       "      <td>Entry level</td>\n",
       "      <td>Contract</td>\n",
       "      <td>Accounting/Auditing and Finance</td>\n",
       "      <td>Staffing and Recruiting</td>\n",
       "      <td>2023-09-08 04:39:40.350584</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/video-data-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3707507616</td>\n",
       "      <td>ICONMA</td>\n",
       "      <td>Cupertino, CA</td>\n",
       "      <td>Process Technician I</td>\n",
       "      <td>\\n\\nProcess Technician I\\n \\n \\n \\n \\n    Loca...</td>\n",
       "      <td></td>\n",
       "      <td>Entry level</td>\n",
       "      <td>Contract</td>\n",
       "      <td>Management and Manufacturing</td>\n",
       "      <td>Staffing and Recruiting</td>\n",
       "      <td>2023-09-08 04:39:40.707149</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/process-tec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       job_id                                            company   \n",
       "2  3715560870  TechFetch.com - On Demand Tech Workforce hirin...  \\\n",
       "3  3708150943                               Tanisha Systems, Inc   \n",
       "4  3712851766                                       SPECTRAFORCE   \n",
       "5  3707550926                   TalentBurst, an Inc 5000 company   \n",
       "6  3708148376                                      Russell Tobin   \n",
       "7  3712609923                                       SPECTRAFORCE   \n",
       "8  3707532523                   TalentBurst, an Inc 5000 company   \n",
       "9  3707507616                                             ICONMA   \n",
       "\n",
       "        location                                      job_title   \n",
       "2   San Jose, CA  Remote - Azure Data Engineer with Data Bricks  \\\n",
       "3   San Jose, CA           Azure Data Engineer with Data Bricks   \n",
       "4  Cupertino, CA                           Video Data Collector   \n",
       "5  Cupertino, CA               Video Data Collector #: 23-15206   \n",
       "6  Cupertino, CA                           Process Technician I   \n",
       "7  Cupertino, CA                           Video Data Collector   \n",
       "8  Cupertino, CA                           Video Data Collector   \n",
       "9  Cupertino, CA                           Process Technician I   \n",
       "\n",
       "                                     job_description   \n",
       "2  \\n        \"ALL our jobs are US based and candi...  \\\n",
       "3  \\n        ob role: \\nAzure Data Engineer with ...   \n",
       "4  \\n\\nJob Title: Video Data Collector\\nDuration:...   \n",
       "5  \\n\\nJob Description\\n Title: Video Data Collec...   \n",
       "6  \\n\\nTitle: Video Data Collector\\nJob Type: 6 m...   \n",
       "7  \\n\\nJob Title: Video Data Collector\\nDuration:...   \n",
       "8  \\n\\nTitle: Video Data Collector\\n \\n Location:...   \n",
       "9  \\n\\nProcess Technician I\\n \\n \\n \\n \\n    Loca...   \n",
       "\n",
       "                                          experience seniority_level   \n",
       "2          \"ALL our jobs are US based and candida...     Entry level  \\\n",
       "3  Required Skills/Qualifications\\n6+ years\\nDesi...     Entry level   \n",
       "4                                                        Entry level   \n",
       "5                                                        Entry level   \n",
       "6                                                        Entry level   \n",
       "7                                                        Entry level   \n",
       "8                                                        Entry level   \n",
       "9                                                        Entry level   \n",
       "\n",
       "  employment_type                      job_function   \n",
       "2       Part-time            Information Technology  \\\n",
       "3        Contract            Information Technology   \n",
       "4        Contract  Art/Creative and Writing/Editing   \n",
       "5        Contract   Accounting/Auditing and Finance   \n",
       "6        Contract      Management and Manufacturing   \n",
       "7       Full-time   Accounting/Auditing and Finance   \n",
       "8        Contract   Accounting/Auditing and Finance   \n",
       "9        Contract      Management and Manufacturing   \n",
       "\n",
       "                      industries               posting_date   \n",
       "2  IT Services and IT Consulting 2023-09-09 04:39:38.138934  \\\n",
       "3       Human Resources Services 2023-09-08 04:39:38.722594   \n",
       "4        Staffing and Recruiting 2023-09-09 04:39:39.023121   \n",
       "5        Staffing and Recruiting 2023-09-09 04:39:39.328467   \n",
       "6        Staffing and Recruiting 2023-09-08 04:39:39.735684   \n",
       "7        Staffing and Recruiting 2023-09-07 04:39:40.045584   \n",
       "8        Staffing and Recruiting 2023-09-08 04:39:40.350584   \n",
       "9        Staffing and Recruiting 2023-09-08 04:39:40.707149   \n",
       "\n",
       "                                                 url  \n",
       "2  https://www.linkedin.com/jobs/view/remote-azur...  \n",
       "3  https://www.linkedin.com/jobs/view/azure-data-...  \n",
       "4  https://www.linkedin.com/jobs/view/video-data-...  \n",
       "5  https://www.linkedin.com/jobs/view/video-data-...  \n",
       "6  https://www.linkedin.com/jobs/view/process-tec...  \n",
       "7  https://www.linkedin.com/jobs/view/video-data-...  \n",
       "8  https://www.linkedin.com/jobs/view/video-data-...  \n",
       "9  https://www.linkedin.com/jobs/view/process-tec...  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_job_info = df_all_job_info[~df_all_job_info['job_id'].isin(list(df_jobs_exists['job_id']))]\n",
    "df_all_job_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert job info (dataframe) into the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_job_info.to_sql('tbl_jobs', engine, 'appuser', if_exists='append', index=False, method=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double Check database insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as conn:\n",
    "    df_check_database = pd.read_sql_query(f'SELECT * FROM tbl_jobs WHERE job_id IN ({\",\".join(job_ids)})', conn)\n",
    "\n",
    "\n",
    "df_check_database.to_html(f\"templates/{string_format(title)}_{string_format(location)}.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3708952912 3714572531]\n",
      "['BDO USA' 'ClearanceJobs']\n",
      "['St Louis, MO' 'St Louis, MO']\n",
      "['Tax Digital Transformation & Innovation Senior Data Engineer'\n",
      " 'Data Scientist with Security Clearance']\n",
      "['\\n\\nJob Description\\nJob Summary:\\nThe Digital Transformation & Innovation (DT&I) Data Manager will work with the Data Engineering and Integration team to help develop and support BDO’s Tax Data Warehouse, manage and maintain data ingestion and Extract/Transform/Load processes, as well as provide support for the Power BI business intelligence platform and enterprise applications supporting the tax practice.\\nThe DT&I Data Manager will build and maintain tax data pipelines to support ad-hoc analytics and business intelligence applications as well as identify valuable development opportunities and ideas for improvement. This role will collaborate closely with data analytics teams to design, develop, and deploy new solutions that support strategic business priorities.\\nJob Duties\\n Develops software that processes, stores, and serves data for use by others \\n Develops large scale data structures and pipelines to organize, collect, and standardize data to generate insights and address reporting needs \\n Writes ETL processes, designs database systems, and develops tools for real time and offline analytic processing \\n Develops and maintains optimal data pipelines into an advanced analytics platform, including design of data flows, procedures, and schedules. Ensures that optimal data pipelines are scalable, repeatable, and secure \\n Troubleshoots software and processes for data consistency and integrity. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards \\n Anticipates and prevents problems and roadblocks before they occur \\n Interacts with internal Tax developers and external peers in Information Technology to exchange ideas and collaborate on data integration solutions \\n Collaborates with data scientists to prepare data for model development \\nSupervisory Responsibilities\\n N/A \\nEducation\\nQualifications, Knowledge, Skills, and Abilities:\\n Bachelor\\'s degree and six (6) or more years of experience in the technology field, required; OR High School Diploma/GED and seven (7) or more years of experience in the technology field, required \\n Bachelor’s degree with a focus in Information Systems, Computer Science, Engineering, Information Technology, or Mathematics, preferred \\nExperience\\n Tax or accounting experience, preferred \\nLicense/Certifications\\n Microsoft Azure Data Engineer, preferred \\nSoftware\\n Advanced experience with SQL, required \\n Experience with Python, required \\n Experience with Linux or Unix, required \\n Data Definition Language (DDL), Data Manipulation Language (DML), views, functions, stored procedures, or performance tuning, preferred \\n Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema Construction, preferred\\n Experience with tax and/or accounting data, preferred \\n Hands on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, preferred \\n Experience with one (1) or more of the following computer languages, preferred:\\n Java \\n C#, C++ \\n Scala \\n Experience with tabular modeling within Power BI or Azure Analysis Services, preferred \\n Experience with Git and DevOps deployment technologies, preferred \\n Experience with one (1) or more of the following, preferred:\\n AI Algorithms/Machine Learning \\n Automation tools such as Data Factory, Data Bricks, Alteryx, etc. \\n Computer Vision based AI technologies \\nLanguage\\n N/A \\nOther Knowledge, Skills, & Abilities\\n Demonstrated ability to work well remotely \\n Solid verbal and written communication skills \\n Strong interpersonal skills, including training/instruction with professionals at all levels \\n Demonstrated sound decision-making skills \\n Ability to work within teams \\n Ability to complete projects independently \\nAbout Us\\nBDO delivers assurance, tax, digital technology solutions and financial advisory services to clients throughout the country and around the globe. We offer numerous industry-specific practices, world-class resources, and an unparalleled commitment to meeting our clients’ needs. We currently serve more than 400 publicly traded domestic and international clients.\\nUnparalleled partner-involvement \\nDeep industry knowledge and participation\\nGeographic coverage across the U.S.\\nCohesive global network \\nFocused capabilities across disciplines\\nBDO brings world-class resources and exceptional service to each and every one of our clients. BDO USA is a member of BDO International, the world’s fifth largest accounting network.\\nBDO offers a competitive Total Rewards package that encompass so much more than – “traditional benefits”. Our wide range of rewards and our employees’ ability to customize rewards to their individual needs are two of the reasons why BDO has been honored with so many workplace awards, including 100 Best Companies for Working Parents, Working Mother 100 Best Companies, Top Entry Level Employer, 2022 National Best & Brightest Companies to Work For and more.\\nSome Examples Of Our Total Rewards Offerings Include\\nCompetitive pay and eligibility for an annual performance bonus. \\nA 401k plan plus an employer match\\nComprehensive, medical, dental, vision, FSA, and prescription insurance from day one\\n Competitive Paid Time Off with daily accrual from day one of employment, plus paid holidays \\nPaid Parental Leave\\nAdoption Assistance\\nFirm paid life insurance\\nWellness programs\\nAdditional offerings include BDO Flex, Group Legal insurance, Pet insurance and Long-Term Care Insurance \\nAbove offerings may be subject to eligibility requirements.\\nClick here to find out more!\\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status.\\n\"BDO USA, P.A. is an EO employer M/F/Veteran/Disability\"\\n\\n'\n",
      " \"\\n        Job Number: R0179261 Data Scientist\\nThe Opportunity: Ever-expanding technology like IoT, machine learning, and artificial intelligence means that there's more structured and unstructured data available today than ever before. As a data engineer, you know that organizing big data can yield pivotal insights when it's gathered from disparate sources. We need an experienced data engineer like you to help our clients find answers in their big data to impact important missions-from fraud detection to cancer research to national intelligence. As a big data engineer at Booz Allen, you'll implement data engineering activities on some of the most mission-driven projects in the industry. You'll deploy and develop pipelines and platforms that organize and make disparate data meaningful. Here, you'll work with and guide a multi-disciplinary team of analysts, data engineers, developers, and data consumers in a fast-paced, Agile environment. You'll use your experience in analytical exploration and data examination while you manage the assessment, design, building, and maintenance of scalable platforms for your clients. Work with us to use big data for good. Join us. The world can't wait. You Have: * 5+ years of experience in application development * 5+ years of experience with designing, developing, operationalizing, and maintaining complex data applications at enterprise scale * 3+ years of experience with creating software for retrieving, parsing, and processing structured and unstructured data * 3+ years of experience with building scalable ETL/ELT workflows for reporting and analytics * Experience creating solutions within a collaborative, cross-functional team environment * Ability to develop scripts and programs for converting various types of data into usable formats and support project team to scale, monitor and operate data platforms * Secret clearance * Bachelor's degree Nice If You Have: * Experience with Python, SQL, Scala, or Java * Experience with UNIX and Linux, including basic commands and Shell scripting * Experience with a public Cloud, including AWS, Microsoft Azure, or Google Cloud * Experience with distributed data and computing tools, including Spark, Databricks, Hadoop, Hive, AWS EMR, or Kafka * Experience working on real-time data and streaming applications * Experience with NoSQL implementation, including MongoDB or Cassandra * Experience with data warehousing using AWS Redshift, MySQL, or Snowflake * Experience with Agile engineering practices * TS/SCI clearance with a polygraph Clearance: Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information; Secret clearance is required. Create Your Career: Grow With Us Your growth matters to us-that's why we offer a variety of ways for you to develop your career. With professional and leadership development opportunities like upskilling programs , tuition reimbursement, mentoring, and firm-sponsored networking, you can chart a unique and fulfilling career path on your own terms. A Place Where You Belong Diverse perspectives cultivate collective ingenuity. Booz Allen's culture of respect, equity, and opportunity means that, here, you are free to bring your whole self to work. With an array of business resource groups and other opportunities for connection, you'll build your community in no time. Support Your Well-Being Our comprehensive benefits package includes wellness programs with HSA contributions, paid holidays, paid parental leave, a generous 401(k) match, and more. With these benefits, plus the option for flexible schedules and remote and hybrid locations, we'll support you as you pursue a balanced, fulfilling life-at work and at home. Your Candidate Journey At Booz Allen, we know our people are what propel us forward, and we value relationships most of all. Here, we've compiled a list of resources so you'll know what to expect as we forge a connection with you during your journey as a candidate with us. Compensation At Booz Allen, we celebrate your contributions, provide you with opportunities and choices, and support your total well-being. Our offerings include health, life, disability, financial, and retirement benefits, as well as paid leave, professional development, tuition assistance, work-life programs, and dependent care. Our recognition awards program acknowledges employees for exceptional performance and superior demonstration of our values. Full-time and part-time employees working at least 20 hours a week on a regular basis are eligible to participate in Booz Allen's benefit programs. Individuals that do not meet the threshold are only eligible for select offerings, not inclusive of health benefits. We encourage you to learn more about our total benefits by visiting the Resource page on our Careers site and reviewing Our Employee Benefits page. Salary at Booz Allen is determined by various factors, including but not limited to location, the individual's particular combination of education, knowledge, skills, competencies, and experience, as well as contract-specific affordability and organizational requirements. The projected compensation range for this position is $73,000.00 to $166,000.00 (annualized USD). The estimate displayed represents the typical salary range for this position and is just one component of Booz Allen's total compensation package for employees. Work Model\\nOur people-first culture prioritizes the benefits of flexibility and collaboration, whether that happens in person or remotely. * If this position is listed as remote or hybrid, you'll periodically work from a Booz Allen or client site facility.\\n If this position is listed as onsite, you'll work with colleagues and clients in person, as needed for the specific role. EEO Commitment We're an equal employment opportunity/affirmative action employer that empowers our people to fearlessly drive change - no matter their race, color, ethnicity, religion, sex (including pregnancy, childbirth, lactation, or related medical conditions), national origin, ancestry, age, marital status, sexual orientation, gender identity and expression, disability, veteran status, military or uniformed service member status, genetic information, or any other status protected by applicable federal, state, local, or international law.\\n\\n\"]\n",
      "[\" Bachelor's degree and six (6) or more years of experience in the technology field, required; OR High School Diploma/GED and seven (7) or more years\"\n",
      " \"The Opportunity: Ever-expanding technology like IoT, machine learning, and artificial intelligence means that there's more structured and unstructured data available today than ever before. As a data engineer, you know that organizing big data can yield pivotal insights when it's gathered from disparate sources. We need an experienced data engineer like you to help our clients find answers in their big data to impact important missions-from fraud detection to cancer research to national intelligence. As a big data engineer at Booz Allen, you'll implement data engineering activities on some of the most mission-driven projects in the industry. You'll deploy and develop pipelines and platforms that organize and make disparate data meaningful. Here, you'll work with and guide a multi-disciplinary team of analysts, data engineers, developers, and data consumers in a fast-paced, Agile environment. You'll use your experience in analytical exploration and data examination while you manage the assessment, design, building, and maintenance of scalable platforms for your clients. Work with us to use big data for good. Join us. The world can't wait. You Have: * 5+ years of experience in application development * 5+ years of experience with designing, developing, operationalizing, and maintaining complex data applications at enterprise scale * 3+ years of experience with creating software for retrieving, parsing, and processing structured and unstructured data * 3+ years\"]\n",
      "['Entry level' 'Entry level']\n",
      "['Full-time' 'Part-time']\n",
      "['Finance and Accounting/Auditing'\n",
      " 'Engineering and Information Technology']\n",
      "['Accounting' 'Defense and Space Manufacturing']\n",
      "['2023-09-09T04:00:51.000000000' '2023-09-07T04:00:52.000000000']\n",
      "['https://www.linkedin.com/jobs/view/tax-digital-transformation-innovation-senior-data-engineer-at-bdo-usa-3708952912?trk=public_jobs_topcard-title'\n",
      " 'https://www.linkedin.com/jobs/view/data-scientist-with-security-clearance-at-clearancejobs-3714572531?trk=public_jobs_topcard-title']\n"
     ]
    }
   ],
   "source": [
    "title = \"Data Engineer\"\n",
    "location = \"St Louis, MO\"\n",
    "\n",
    "\n",
    "with engine.connect() as conn:\n",
    "        df_results = pd.read_sql_query(\n",
    "            f\"SELECT * FROM tbl_jobs WHERE location like '%{location}%'\", conn)\n",
    "        \n",
    "for value in df_results.items():\n",
    "    print(value[1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3708952912\n",
      "BDO USA\n",
      "St Louis, MO\n",
      "Tax Digital Transformation & Innovation Senior Data Engineer\n",
      "\n",
      "\n",
      "Job Description\n",
      "Job Summary:\n",
      "The Digital Transformation & Innovation (DT&I) Data Manager will work with the Data Engineering and Integration team to help develop and support BDO’s Tax Data Warehouse, manage and maintain data ingestion and Extract/Transform/Load processes, as well as provide support for the Power BI business intelligence platform and enterprise applications supporting the tax practice.\n",
      "The DT&I Data Manager will build and maintain tax data pipelines to support ad-hoc analytics and business intelligence applications as well as identify valuable development opportunities and ideas for improvement. This role will collaborate closely with data analytics teams to design, develop, and deploy new solutions that support strategic business priorities.\n",
      "Job Duties\n",
      " Develops software that processes, stores, and serves data for use by others \n",
      " Develops large scale data structures and pipelines to organize, collect, and standardize data to generate insights and address reporting needs \n",
      " Writes ETL processes, designs database systems, and develops tools for real time and offline analytic processing \n",
      " Develops and maintains optimal data pipelines into an advanced analytics platform, including design of data flows, procedures, and schedules. Ensures that optimal data pipelines are scalable, repeatable, and secure \n",
      " Troubleshoots software and processes for data consistency and integrity. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards \n",
      " Anticipates and prevents problems and roadblocks before they occur \n",
      " Interacts with internal Tax developers and external peers in Information Technology to exchange ideas and collaborate on data integration solutions \n",
      " Collaborates with data scientists to prepare data for model development \n",
      "Supervisory Responsibilities\n",
      " N/A \n",
      "Education\n",
      "Qualifications, Knowledge, Skills, and Abilities:\n",
      " Bachelor's degree and six (6) or more years of experience in the technology field, required; OR High School Diploma/GED and seven (7) or more years of experience in the technology field, required \n",
      " Bachelor’s degree with a focus in Information Systems, Computer Science, Engineering, Information Technology, or Mathematics, preferred \n",
      "Experience\n",
      " Tax or accounting experience, preferred \n",
      "License/Certifications\n",
      " Microsoft Azure Data Engineer, preferred \n",
      "Software\n",
      " Advanced experience with SQL, required \n",
      " Experience with Python, required \n",
      " Experience with Linux or Unix, required \n",
      " Data Definition Language (DDL), Data Manipulation Language (DML), views, functions, stored procedures, or performance tuning, preferred \n",
      " Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema Construction, preferred\n",
      " Experience with tax and/or accounting data, preferred \n",
      " Hands on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, preferred \n",
      " Experience with one (1) or more of the following computer languages, preferred:\n",
      " Java \n",
      " C#, C++ \n",
      " Scala \n",
      " Experience with tabular modeling within Power BI or Azure Analysis Services, preferred \n",
      " Experience with Git and DevOps deployment technologies, preferred \n",
      " Experience with one (1) or more of the following, preferred:\n",
      " AI Algorithms/Machine Learning \n",
      " Automation tools such as Data Factory, Data Bricks, Alteryx, etc. \n",
      " Computer Vision based AI technologies \n",
      "Language\n",
      " N/A \n",
      "Other Knowledge, Skills, & Abilities\n",
      " Demonstrated ability to work well remotely \n",
      " Solid verbal and written communication skills \n",
      " Strong interpersonal skills, including training/instruction with professionals at all levels \n",
      " Demonstrated sound decision-making skills \n",
      " Ability to work within teams \n",
      " Ability to complete projects independently \n",
      "About Us\n",
      "BDO delivers assurance, tax, digital technology solutions and financial advisory services to clients throughout the country and around the globe. We offer numerous industry-specific practices, world-class resources, and an unparalleled commitment to meeting our clients’ needs. We currently serve more than 400 publicly traded domestic and international clients.\n",
      "Unparalleled partner-involvement \n",
      "Deep industry knowledge and participation\n",
      "Geographic coverage across the U.S.\n",
      "Cohesive global network \n",
      "Focused capabilities across disciplines\n",
      "BDO brings world-class resources and exceptional service to each and every one of our clients. BDO USA is a member of BDO International, the world’s fifth largest accounting network.\n",
      "BDO offers a competitive Total Rewards package that encompass so much more than – “traditional benefits”. Our wide range of rewards and our employees’ ability to customize rewards to their individual needs are two of the reasons why BDO has been honored with so many workplace awards, including 100 Best Companies for Working Parents, Working Mother 100 Best Companies, Top Entry Level Employer, 2022 National Best & Brightest Companies to Work For and more.\n",
      "Some Examples Of Our Total Rewards Offerings Include\n",
      "Competitive pay and eligibility for an annual performance bonus. \n",
      "A 401k plan plus an employer match\n",
      "Comprehensive, medical, dental, vision, FSA, and prescription insurance from day one\n",
      " Competitive Paid Time Off with daily accrual from day one of employment, plus paid holidays \n",
      "Paid Parental Leave\n",
      "Adoption Assistance\n",
      "Firm paid life insurance\n",
      "Wellness programs\n",
      "Additional offerings include BDO Flex, Group Legal insurance, Pet insurance and Long-Term Care Insurance \n",
      "Above offerings may be subject to eligibility requirements.\n",
      "Click here to find out more!\n",
      "All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status.\n",
      "\"BDO USA, P.A. is an EO employer M/F/Veteran/Disability\"\n",
      "\n",
      "\n",
      " Bachelor's degree and six (6) or more years of experience in the technology field, required; OR High School Diploma/GED and seven (7) or more years\n",
      "Entry level\n",
      "Full-time\n",
      "Finance and Accounting/Auditing\n",
      "Accounting\n",
      "2023-09-09 04:00:51\n",
      "https://www.linkedin.com/jobs/view/tax-digital-transformation-innovation-senior-data-engineer-at-bdo-usa-3708952912?trk=public_jobs_topcard-title\n",
      "3714572531\n",
      "ClearanceJobs\n",
      "St Louis, MO\n",
      "Data Scientist with Security Clearance\n",
      "\n",
      "        Job Number: R0179261 Data Scientist\n",
      "The Opportunity: Ever-expanding technology like IoT, machine learning, and artificial intelligence means that there's more structured and unstructured data available today than ever before. As a data engineer, you know that organizing big data can yield pivotal insights when it's gathered from disparate sources. We need an experienced data engineer like you to help our clients find answers in their big data to impact important missions-from fraud detection to cancer research to national intelligence. As a big data engineer at Booz Allen, you'll implement data engineering activities on some of the most mission-driven projects in the industry. You'll deploy and develop pipelines and platforms that organize and make disparate data meaningful. Here, you'll work with and guide a multi-disciplinary team of analysts, data engineers, developers, and data consumers in a fast-paced, Agile environment. You'll use your experience in analytical exploration and data examination while you manage the assessment, design, building, and maintenance of scalable platforms for your clients. Work with us to use big data for good. Join us. The world can't wait. You Have: * 5+ years of experience in application development * 5+ years of experience with designing, developing, operationalizing, and maintaining complex data applications at enterprise scale * 3+ years of experience with creating software for retrieving, parsing, and processing structured and unstructured data * 3+ years of experience with building scalable ETL/ELT workflows for reporting and analytics * Experience creating solutions within a collaborative, cross-functional team environment * Ability to develop scripts and programs for converting various types of data into usable formats and support project team to scale, monitor and operate data platforms * Secret clearance * Bachelor's degree Nice If You Have: * Experience with Python, SQL, Scala, or Java * Experience with UNIX and Linux, including basic commands and Shell scripting * Experience with a public Cloud, including AWS, Microsoft Azure, or Google Cloud * Experience with distributed data and computing tools, including Spark, Databricks, Hadoop, Hive, AWS EMR, or Kafka * Experience working on real-time data and streaming applications * Experience with NoSQL implementation, including MongoDB or Cassandra * Experience with data warehousing using AWS Redshift, MySQL, or Snowflake * Experience with Agile engineering practices * TS/SCI clearance with a polygraph Clearance: Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information; Secret clearance is required. Create Your Career: Grow With Us Your growth matters to us-that's why we offer a variety of ways for you to develop your career. With professional and leadership development opportunities like upskilling programs , tuition reimbursement, mentoring, and firm-sponsored networking, you can chart a unique and fulfilling career path on your own terms. A Place Where You Belong Diverse perspectives cultivate collective ingenuity. Booz Allen's culture of respect, equity, and opportunity means that, here, you are free to bring your whole self to work. With an array of business resource groups and other opportunities for connection, you'll build your community in no time. Support Your Well-Being Our comprehensive benefits package includes wellness programs with HSA contributions, paid holidays, paid parental leave, a generous 401(k) match, and more. With these benefits, plus the option for flexible schedules and remote and hybrid locations, we'll support you as you pursue a balanced, fulfilling life-at work and at home. Your Candidate Journey At Booz Allen, we know our people are what propel us forward, and we value relationships most of all. Here, we've compiled a list of resources so you'll know what to expect as we forge a connection with you during your journey as a candidate with us. Compensation At Booz Allen, we celebrate your contributions, provide you with opportunities and choices, and support your total well-being. Our offerings include health, life, disability, financial, and retirement benefits, as well as paid leave, professional development, tuition assistance, work-life programs, and dependent care. Our recognition awards program acknowledges employees for exceptional performance and superior demonstration of our values. Full-time and part-time employees working at least 20 hours a week on a regular basis are eligible to participate in Booz Allen's benefit programs. Individuals that do not meet the threshold are only eligible for select offerings, not inclusive of health benefits. We encourage you to learn more about our total benefits by visiting the Resource page on our Careers site and reviewing Our Employee Benefits page. Salary at Booz Allen is determined by various factors, including but not limited to location, the individual's particular combination of education, knowledge, skills, competencies, and experience, as well as contract-specific affordability and organizational requirements. The projected compensation range for this position is $73,000.00 to $166,000.00 (annualized USD). The estimate displayed represents the typical salary range for this position and is just one component of Booz Allen's total compensation package for employees. Work Model\n",
      "Our people-first culture prioritizes the benefits of flexibility and collaboration, whether that happens in person or remotely. * If this position is listed as remote or hybrid, you'll periodically work from a Booz Allen or client site facility.\n",
      " If this position is listed as onsite, you'll work with colleagues and clients in person, as needed for the specific role. EEO Commitment We're an equal employment opportunity/affirmative action employer that empowers our people to fearlessly drive change - no matter their race, color, ethnicity, religion, sex (including pregnancy, childbirth, lactation, or related medical conditions), national origin, ancestry, age, marital status, sexual orientation, gender identity and expression, disability, veteran status, military or uniformed service member status, genetic information, or any other status protected by applicable federal, state, local, or international law.\n",
      "\n",
      "\n",
      "The Opportunity: Ever-expanding technology like IoT, machine learning, and artificial intelligence means that there's more structured and unstructured data available today than ever before. As a data engineer, you know that organizing big data can yield pivotal insights when it's gathered from disparate sources. We need an experienced data engineer like you to help our clients find answers in their big data to impact important missions-from fraud detection to cancer research to national intelligence. As a big data engineer at Booz Allen, you'll implement data engineering activities on some of the most mission-driven projects in the industry. You'll deploy and develop pipelines and platforms that organize and make disparate data meaningful. Here, you'll work with and guide a multi-disciplinary team of analysts, data engineers, developers, and data consumers in a fast-paced, Agile environment. You'll use your experience in analytical exploration and data examination while you manage the assessment, design, building, and maintenance of scalable platforms for your clients. Work with us to use big data for good. Join us. The world can't wait. You Have: * 5+ years of experience in application development * 5+ years of experience with designing, developing, operationalizing, and maintaining complex data applications at enterprise scale * 3+ years of experience with creating software for retrieving, parsing, and processing structured and unstructured data * 3+ years\n",
      "Entry level\n",
      "Part-time\n",
      "Engineering and Information Technology\n",
      "Defense and Space Manufacturing\n",
      "2023-09-07 04:00:52\n",
      "https://www.linkedin.com/jobs/view/data-scientist-with-security-clearance-at-clearancejobs-3714572531?trk=public_jobs_topcard-title\n",
      "3708148376\n",
      "Russell Tobin\n",
      "Cupertino, CA\n",
      "Process Technician I\n",
      "\n",
      "\n",
      "Title: Video Data Collector\n",
      "Job Type: 6 months Contract\n",
      "Location: Cupertino, CA\n",
      "Pay Rate Range: $25 – $29.89 per hour\n",
      "Job Summary:\n",
      "The Video Engineering group is looking for a Video Data Collector. The candidate for this position will collaborate with data engineer, HW/SW research engineers, and project managers to develop and execute small-scale data gathering efforts. We work on multiple, related projects requiring creativity and resourcefulness. You will be able to demonstrate your skills and expertise as we work together to innovate. We are an unusual team in how we operate, and we are looking for creative individual and anyone with room to grow to join us. We are unusual in the sense that our work fills in a gap between Algorithm teams, Quality Assurance (QA), and User Studies. Do you think differently? Are you passionate about meaningful innovation? We often think, tinker, debate, and investigate fun problems with a high level of transparency. We also tackle a diversity of problems; as a result, work is entertaining, captivating, and exciting.\n",
      "Responsibilities include:\n",
      "Follow the protocols to collect the video data by using DSLR and iPhone\n",
      "Work closely with engineers to support process work\n",
      "Ability to organize and execute complex, detailed capture procedures/sceneries setup\n",
      "Offload the data from devices and upload to internal platform by using internal tools\n",
      "Maintain a clean, orderly, and safe work environment and documentation Video Data Collector\n",
      "Key Qualifications:\n",
      "Must be able to work independently as well with other team members\n",
      "Able to follow standard operating procedures and safety protocols\n",
      "Self-starter with time management capability and a can[1]do attitude\n",
      "Excellent verbal and written communication skills\n",
      "Experience with iOS, Mac OS and other productivity tools\n",
      "Experience with DSLR concept and usage\n",
      "Education:\n",
      "Bachelors degree required\n",
      "#CB\n",
      "Rate/Salary: $25 – $29.89 per hour\n",
      "      \n",
      "None\n",
      "Entry level\n",
      "Contract\n",
      "Management and Manufacturing\n",
      "Staffing and Recruiting\n",
      "2023-09-08 04:39:39\n",
      "https://www.linkedin.com/jobs/view/process-technician-i-at-russell-tobin-3708148376?trk=public_jobs_topcard-title\n",
      "3712609923\n",
      "SPECTRAFORCE\n",
      "Cupertino, CA\n",
      "Video Data Collector\n",
      "\n",
      "\n",
      "Job Title: Video Data Collector\n",
      "Duration: 6 months\n",
      "Location: Cupertino, CA 95014\n",
      "PR: $29.88/hr\n",
      "Job Summary\n",
      "The Video Engineering group is looking for a Video Data Collector. The candidate for this position will collaborate with data engineer, HW/SW research engineers, and project managers to develop and execute small-scale data gathering efforts. We work on multiple, related projects requiring creativity and resourcefulness. You will be able to demonstrate your skills and expertise as we work together to innovate. We are an unusual team in how we operate, and we are looking for creative individual and anyone with room to grow to join us. We are unusual in the sense that our work fills in a gap between Algorithm teams, Quality Assurance (QA), and User Studies. Do you think differently? Are you passionate about meaningful innovation? We often think, tinker, debate, and investigate fun problems with a high level of transparency. We also tackle a diversity of problems; as a result, work is entertaining, captivating, and exciting.\n",
      "Responsibilities\n",
      "Follow the protocols to collect the video data by using DSLR and iPhone\n",
      "Work closely with engineers to support process work\n",
      "Ability to organize and execute complex, detailed capture procedures/sceneries setup\n",
      "Offload the data from devices and upload to internal platform by using internal tools\n",
      "Maintain a clean, orderly, and safe work environment and documentation\n",
      "Key Qualifications\n",
      "Must be able to work independently as well with other team members\n",
      "Able to follow standard operating procedures and safety protocols\n",
      "Self-starter with time management capability and a can- do attitude\n",
      "Excellent verbal and written communication skills\n",
      "Experience with iOS, Mac OS and other productivity tools\n",
      "Experience with DSLR concept and usage\n",
      "Education\n",
      "Bachelors degree required\n",
      "Language Skills\n",
      "English required\n",
      "\n",
      "\n",
      "None\n",
      "Entry level\n",
      "Full-time\n",
      "Accounting/Auditing and Finance\n",
      "Staffing and Recruiting\n",
      "2023-09-07 04:39:40\n",
      "https://www.linkedin.com/jobs/view/video-data-collector-at-spectraforce-3712609923?trk=public_jobs_topcard-title\n",
      "3707532523\n",
      "TalentBurst, an Inc 5000 company\n",
      "Cupertino, CA\n",
      "Video Data Collector\n",
      "\n",
      "\n",
      "Title: Video Data Collector\n",
      " \n",
      " Location: Cupertino, CA 95014 \n",
      " \n",
      " Duration: 6 Months + \n",
      " \n",
      " 25198334\n",
      " \n",
      " 100 % ONSITE \n",
      " \n",
      " \n",
      " Job Summary\n",
      " The Video Engineering group is looking for a Video Data Collector. The candidate for this position will collaborate with data engineer, HW/SW research engineers, and project managers to develop and execute small-scale data gathering efforts. We work on multiple, related projects requiring creativity and resourcefulness. You will be able to demonstrate your skills and expertise as we work together to innovate. We are an unusual team in how we operate, and we are looking for creative individual and anyone with room to grow to join us. We are unusual in the sense that our work fills in a gap between Algorithm teams, Quality Assurance (QA), and User Studies. Do you think differently? Are you passionate about meaningful innovation? We often think, tinker, debate, and investigate fun problems with a high level of transparency. We also tackle a diversity of problems; as a result, work is entertaining, captivating, and exciting.\n",
      " \n",
      " Responsibilities include.\n",
      " \n",
      " Follow the protocols to collect the video data by using DSLR and iPhone.\n",
      " \n",
      " Work closely with engineers to support process work.\n",
      " \n",
      " Ability to organize and execute complex, detailed capture procedures/sceneries setup.\n",
      " \n",
      " Offload the data from devices and upload to internal platform by using internal tools.\n",
      " \n",
      " Maintain a clean, orderly, and safe work environment and documentation.\n",
      " \n",
      " \n",
      " \n",
      " Key Qualifications\n",
      " \n",
      " Must be able to work independently as well with other team members.\n",
      " \n",
      " Able to follow standard operating procedures and safety protocols.\n",
      " \n",
      " Self-starter with time management capability and a can do attitude.\n",
      " \n",
      " Excellent verbal and written communication skills\n",
      " \n",
      " Experience with iOS, Mac OS and other productivity tools\n",
      " \n",
      " Experience with DSLR concept and usage\n",
      " \n",
      " \n",
      " \n",
      " Education\n",
      " \n",
      " Bachelors degree required\n",
      " \n",
      " \n",
      " \n",
      " Language Skills\n",
      " \n",
      " English required.\n",
      " \n",
      " \n",
      "  Required Skills:\n",
      " MACOS\n",
      " PROCESS TECHNICIAN\n",
      " TIME MANAGEMENT\n",
      " SELF-STARTER\n",
      " \n",
      " Additional Skills:\n",
      " EXCELLENT VERBAL AND WRITTEN COMMUNICATION SKILLS\n",
      " HIGH ACCURACY\n",
      " PROCESS DEVELOPMENT\n",
      " \n",
      " \n",
      " TB_HL\n",
      "\n",
      "\n",
      "None\n",
      "Entry level\n",
      "Contract\n",
      "Accounting/Auditing and Finance\n",
      "Staffing and Recruiting\n",
      "2023-09-08 04:39:40\n",
      "https://www.linkedin.com/jobs/view/video-data-collector-at-talentburst-an-inc-5000-company-3707532523?trk=public_jobs_topcard-title\n",
      "3707507616\n",
      "ICONMA\n",
      "Cupertino, CA\n",
      "Process Technician I\n",
      "\n",
      "\n",
      "Process Technician I\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "    Location: Cupertino, CA\n",
      " \n",
      " \n",
      "    Duration: 6 Months\n",
      " \n",
      " \n",
      " \n",
      "    Description:\n",
      "    The Video Engineering group is looking for a Video Data Collector.\n",
      "    The candidate for this position will collaborate with data engineer, HW/SW research engineers, and project managers to develop and execute small-scale data gathering efforts.\n",
      "    We work on multiple, related projects requiring creativity and resourcefulness.\n",
      "    You will be able to demonstrate your skills and expertise as we work together to innovate.\n",
      "    We are an unusual team in how we operate, and we are looking for creative individual and anyone with room to grow to join us.\n",
      "    We are unusual in the sense that our work fills in a gap between Algorithm teams, Quality Assurance (QA), and User Studies.\n",
      " \n",
      " \n",
      "    Do you think differently: \n",
      "    Are you passionate about meaningful innovation: We often think, tinker, debate, and investigate fun problems with a high level of transparency.\n",
      "    We also tackle a diversity of problems; as a result, work is entertaining, captivating, and exciting.\n",
      " \n",
      " \n",
      "    Responsibilities include\n",
      "    Follow the protocols to collect the video data by using DSLR and iPhone\n",
      "    Work closely with engineers to support process work\n",
      "    Ability to organize and execute complex, detailed capture procedures/sceneries setup\n",
      "    Offload the data from devices and upload to internal platform by using internal tools\n",
      "    Maintain a clean, orderly, and safe work environment and documentation Video Data Collector\n",
      " \n",
      " \n",
      "    Key Qualifications\n",
      "    Must be able to work independently as well with other team members\n",
      "    Able to follow standard operating procedures and safety protocols\n",
      "    Self-starter with time management capability and a can do attitude\n",
      "    Excellent verbal and written communication skills\n",
      "    Experience with iOS, Mac OS and other productivity tools\n",
      "    Experience with DSLR concept and usage\n",
      " \n",
      " \n",
      "    Education\n",
      "    Bachelors degree required\n",
      " \n",
      " \n",
      "    Language Skills\n",
      "    English required\n",
      "    Job Posting Selection\n",
      "    External\n",
      "\n",
      "\n",
      "None\n",
      "Entry level\n",
      "Contract\n",
      "Management and Manufacturing\n",
      "Staffing and Recruiting\n",
      "2023-09-08 04:39:40\n",
      "https://www.linkedin.com/jobs/view/process-technician-i-at-iconma-3707507616?trk=public_jobs_topcard-title\n",
      "3708984293\n",
      "ShiftCode Analytics, Inc.\n",
      "Chicago, IL\n",
      "Data Engineer : Vizient\n",
      "\n",
      "\n",
      "Hybrid 2-3 days onsite\n",
      "Interview process : Two rounds one video and then onsite\n",
      "NEED LOCALS ONLY\n",
      "NEED DL COPY\n",
      "NEED SSN ( LAST 4 DIGITS )\n",
      "NO VIOP OR GOOGLE VOICE NUMBERS\n",
      "NBEED LINKEDIN\n",
      " Skill Set Category: Data Engineer \n",
      " Must haves \n",
      " Experience with Python & SQL \n",
      " Experience working with large data sets \n",
      " Experience with Azure Databricks and Data Factory \n",
      " Experience with Apache Spark \n",
      "Skills\n",
      " 5+ experience hands-on experience in data engineering (developing, maintaining, testing, analyzing and evaluating data. \n",
      " Must have experience with Python and SQL \n",
      " Experience working with large datasets \n",
      " Experience with loading data using any MPP architecture \n",
      " Experience working with Azure, Databricks and Data Factory \n",
      " Responsible for building and maintaining the logic and functionality needed to power the user-facing components of a website \n",
      " Experience working with algorithms in translating business logic to procedural code \n",
      " Experience with Big Data preferably Apache Spark \n",
      " Strongly prefer experience with C#\n",
      "\n",
      "\n",
      "None\n",
      "Entry level\n",
      "Full-time\n",
      "Information Technology\n",
      "Information Technology & Services\n",
      "2023-09-09 04:04:26\n",
      "https://www.linkedin.com/jobs/view/data-engineer-vizient-at-shiftcode-analytics-inc-3708984293?trk=public_jobs_topcard-title\n",
      "3674224518\n",
      "Old National Bank\n",
      "Chicago, IL\n",
      "Informatica IICS Data Engineer\n",
      "\n",
      "\n",
      "Overview\n",
      "Old National Bank has been serving clients and communities since 1834. With $48 billion in total assets, we are a regional powerhouse deeply rooted in the communities we serve. As a trusted partner, we thrive on helping our clients achieve their goals and dreams, and we are committed to social responsibility and investing in our communities through volunteering and charitable giving.\n",
      "Our team members a re our greatest asset, and we continually invest in their growth and development. We offer a variety of Impact Network Groups led by team members who are passionate about driving engagement, creating awareness of diverse backgrounds and experiences, and building inclusion across the organization.\n",
      "We are currently seeking a Data Engineer responsible for the development and implementation of technical solutions. This role leverages technical expertise in database applications, ETL processes, and automation in support of internal and external customers data exchange and integration needs. Participate and deliver Salesforce integration and data warehouse project and later work as the main ETL support member after the go-live. We are not currently sponsoring H-1. We require you to live near and work at our offfices in Chicago near O'Hare airport, close to many suburbs, or at Eden Prairie Minnesota.\n",
      "REQUIRED EDUCATION, EXPERIENCE AND SKILLS:\n",
      ".\n",
      " Over 5 years of experience with Informatica developing ETL and recent experience with IICS. We prefer 3 years of experience with SSIS, and some knowledge of Python. \n",
      " Expertise in designing and implementing Relational Database mode as per business needs \n",
      " Must have experience in Database Development, Design, Analysis, SQL Servers 2015-2019 in Development, Testing and Production Environments with various business domains \n",
      " Experienced in extracting, transforming and loading ETL data from excel, flat file. C and C# used for scripting in Script Tasking in SSIS to control the flow of SSIS package \n",
      " Very good working experience of T-SQL. Proficiency in complex stored procedures, User Defined Functions (UDF), Database Triggers, using tools like SQL Profiler and Database Tuning Advisor (DTA) \n",
      " Implemented incremental load for many SSIS packages. Experience in error handling, debugging, error logging for production support in SSIS. Experience in bulk importing CSV, XML and Flat files data using Bulk copy program (BCP) \n",
      " Hands on experience in performance tuning and query optimization including updating statistics, rebuilding and re-organizing indexes \n",
      " Experienced in normalizing and de-normalizing the tables and maintaining referential integrity by using triggers and primary and foreign keys \n",
      " Ability to work independently and be able to collaborate and guide other team members \n",
      " Strong written and oral communication, experience on reproving and presenting new design and solutions to senior leadership \n",
      " Working knowledge of MS SSMS 2012 to 2019 \n",
      " Working knowledge of analytical and data visualization technology such as Power BI, R Services \n",
      " Working knowledge of cloud based data architectures leveraging PAAS, IAAS, DAAS etc.. \n",
      " Familiarity with MS DQM, MDM, Big Data concepts Works well independently; capable of self-motivating and managing. \n",
      "Duties/Responsibilities\n",
      " Design, development, and maintenance (enhancements and maintenance) of ETL processes and business intelligence solutions using the IICS and SQL Server Relational Database technologies. \n",
      " Design and development of processes for various types of data (potentially large datasets and disparate data sources that require transformation and cleansing to become a usable data set.) \n",
      " Monitor production jobs, maintain, and enhance legacy ETL processes on a regular basis. Improve slow running jobs with the help of redesign and better ETL processes to meet business needs. \n",
      " Take directions, follow recommendations provided by data architecture, Info Security and data governance organization. \n",
      " Confer with relevant team members where necessary. Study systems flow, data usage, and work processes. \n",
      " Deliver functional work product that has been thoroughly tested. \n",
      " Follow development team’s SDLC process. Accurately estimate time required to complete projects and tasks. \n",
      " Meet mutually agreed upon deadlines for completion of modules throughout the program development. \n",
      " Work very closely with the existing DBA to design, model, develop, and maintain existing and new SQL databases objects required for all business solutions \n",
      "Key Competencies for Position \n",
      " Works well independently; capable of self-motivating and managing. \n",
      " Ability to work independently and be able to collaborate and guide other team members. \n",
      " Strong written and oral communication, experience on reproving and presenting new design and solutions to senior leadership. \n",
      " Old National is proud to be an equal opportunity employer focused on fostering an inclusive workplace and committed to hiring a workforce comprised of diverse backgrounds, cultures and thinking styles. \n",
      "As such, all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, protected veteran status, status as a qualified individual with disability, sexual orientation, gender identity or any other characteristic protected by law.\n",
      "We do not accept resumes from external staffing agencies or independent recruiters for any of our openings unless we have an agreement signed by the Head of Talent Acquisition, SVP, to fill a specific position\n",
      " Our culture is firmly rooted in our core values. \n",
      " We are optimistic. We are collaborative. We are inclusive. We are agile. We are ethical. \n",
      " We are Old National Bank. Join our team!\n",
      "\n",
      "\n",
      " Over 5 years of experience with Informatica developing ETL and recent experience with IICS. We prefer 3 years\n",
      "Entry level\n",
      "Full-time\n",
      "Information Technology\n",
      "Banking\n",
      "2023-09-07 04:04:26\n",
      "https://www.linkedin.com/jobs/view/informatica-iics-data-engineer-at-old-national-bank-3674224518?trk=public_jobs_topcard-title\n",
      "3676035101\n",
      "Old National Bank\n",
      "Tinley Park, IL\n",
      "Informatica IICS Data Engineer\n",
      "\n",
      "\n",
      "Overview\n",
      "Old National Bank has been serving clients and communities since 1834. With $48 billion in total assets, we are a regional powerhouse deeply rooted in the communities we serve. As a trusted partner, we thrive on helping our clients achieve their goals and dreams, and we are committed to social responsibility and investing in our communities through volunteering and charitable giving.\n",
      "Our team members a re our greatest asset, and we continually invest in their growth and development. We offer a variety of Impact Network Groups led by team members who are passionate about driving engagement, creating awareness of diverse backgrounds and experiences, and building inclusion across the organization.\n",
      "We are currently seeking a Data Engineer responsible for the development and implementation of technical solutions. This role leverages technical expertise in database applications, ETL processes, and automation in support of internal and external customers data exchange and integration needs. Participate and deliver Salesforce integration and data warehouse project and later work as the main ETL support member after the go-live. We are not currently sponsoring H-1. We require you to live near and work at our offfices in Chicago near O'Hare airport, close to many suburbs, or at Eden Prairie Minnesota.\n",
      "REQUIRED EDUCATION, EXPERIENCE AND SKILLS:\n",
      ".\n",
      " Over 5 years of experience with Informatica developing ETL and recent experience with IICS. We prefer 3 years of experience with SSIS, and some knowledge of Python. \n",
      " Expertise in designing and implementing Relational Database mode as per business needs \n",
      " Must have experience in Database Development, Design, Analysis, SQL Servers 2015-2019 in Development, Testing and Production Environments with various business domains \n",
      " Experienced in extracting, transforming and loading ETL data from excel, flat file. C and C# used for scripting in Script Tasking in SSIS to control the flow of SSIS package \n",
      " Very good working experience of T-SQL. Proficiency in complex stored procedures, User Defined Functions (UDF), Database Triggers, using tools like SQL Profiler and Database Tuning Advisor (DTA) \n",
      " Implemented incremental load for many SSIS packages. Experience in error handling, debugging, error logging for production support in SSIS. Experience in bulk importing CSV, XML and Flat files data using Bulk copy program (BCP) \n",
      " Hands on experience in performance tuning and query optimization including updating statistics, rebuilding and re-organizing indexes \n",
      " Experienced in normalizing and de-normalizing the tables and maintaining referential integrity by using triggers and primary and foreign keys \n",
      " Ability to work independently and be able to collaborate and guide other team members \n",
      " Strong written and oral communication, experience on reproving and presenting new design and solutions to senior leadership \n",
      " Working knowledge of MS SSMS 2012 to 2019 \n",
      " Working knowledge of analytical and data visualization technology such as Power BI, R Services \n",
      " Working knowledge of cloud based data architectures leveraging PAAS, IAAS, DAAS etc.. \n",
      " Familiarity with MS DQM, MDM, Big Data concepts Works well independently; capable of self-motivating and managing. \n",
      "Duties/Responsibilities\n",
      " Design, development, and maintenance (enhancements and maintenance) of ETL processes and business intelligence solutions using the IICS and SQL Server Relational Database technologies. \n",
      " Design and development of processes for various types of data (potentially large datasets and disparate data sources that require transformation and cleansing to become a usable data set.) \n",
      " Monitor production jobs, maintain, and enhance legacy ETL processes on a regular basis. Improve slow running jobs with the help of redesign and better ETL processes to meet business needs. \n",
      " Take directions, follow recommendations provided by data architecture, Info Security and data governance organization. \n",
      " Confer with relevant team members where necessary. Study systems flow, data usage, and work processes. \n",
      " Deliver functional work product that has been thoroughly tested. \n",
      " Follow development team’s SDLC process. Accurately estimate time required to complete projects and tasks. \n",
      " Meet mutually agreed upon deadlines for completion of modules throughout the program development. \n",
      " Work very closely with the existing DBA to design, model, develop, and maintain existing and new SQL databases objects required for all business solutions \n",
      "Key Competencies for Position \n",
      " Works well independently; capable of self-motivating and managing. \n",
      " Ability to work independently and be able to collaborate and guide other team members. \n",
      " Strong written and oral communication, experience on reproving and presenting new design and solutions to senior leadership. \n",
      " Old National is proud to be an equal opportunity employer focused on fostering an inclusive workplace and committed to hiring a workforce comprised of diverse backgrounds, cultures and thinking styles. \n",
      "As such, all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, protected veteran status, status as a qualified individual with disability, sexual orientation, gender identity or any other characteristic protected by law.\n",
      "We do not accept resumes from external staffing agencies or independent recruiters for any of our openings unless we have an agreement signed by the Head of Talent Acquisition, SVP, to fill a specific position\n",
      " Our culture is firmly rooted in our core values. \n",
      " We are optimistic. We are collaborative. We are inclusive. We are agile. We are ethical. \n",
      " We are Old National Bank. Join our team!\n",
      "\n",
      "\n",
      " Over 5 years of experience with Informatica developing ETL and recent experience with IICS. We prefer 3 years\n",
      "Entry level\n",
      "Full-time\n",
      "Information Technology\n",
      "Banking\n",
      "2023-09-08 04:04:27\n",
      "https://www.linkedin.com/jobs/view/informatica-iics-data-engineer-at-old-national-bank-3676035101?trk=public_jobs_topcard-title\n",
      "3610297973\n",
      "Medline Industries, LP\n",
      "Northfield, IL\n",
      "Azure Data Engineer/Administrator\n",
      "\n",
      "        Medline Industries continues to grow, and grow, and grow. In fact, we've enjoyed DOUBLE DIGIT growth in 54 of the past 55 years! And we are AGAIN named as a Chicago Tribune Top Employer! Doesn't that sound like the kind of place you'd want to join?\n",
      "The “ETL Administrator/Developer” will play a key role in administering ETL platforms and owning ongoing support of ETL applications & solutions. This role will be a critical individual performer that will contribute to ensuring platform availability, application availability, production support, capacity management, performance tuning, and system stability. The ideal candidate would have the right combination of administration expertise and command along with demonstrated application development/maintenance skills as it relates to quickly analyzing, understanding pipelines/processes, and proposing solutions for design/performance improvements, process improvements, or production application stability. Further, the role will also involve implementing such improvements on existing solutions and/or developing new ETL solutions to address internal BI use cases relative to the overall BI/DWH strategy. The following background and experience are critical for the role.\n",
      "Comprehensive understanding and experience with Azure services and platforms, including but not limited to ADLS, Azure Data Factory, and Azure Synapse\n",
      "Strong understanding of Data Warehousing (DWH) principles, including ETL/ELT frameworks, architectures, and methodologies in both On-Premises and Cloud environments.\n",
      "Demonstrated experience in administering ETL tools/platforms (Talend) and developing/debugging ETL jobs in On-Prem and Cloud environments (Azure preferred) is a compelling differentiator.\n",
      "Strong Production Support experience with skills in debugging, troubleshooting, root cause analysis, and problem resolution being critical to this role\n",
      "Advanced working experience in SQL.\n",
      "Demonstrated ability to work in a team environment that requires quick turnaround and quality outcomes. Strong communication skills with the ability to effectively communicate with peers, management, partners, and clients.\n",
      "Responsibilities:\n",
      "Continuously optimize, enhance, maintain, support and monitor all data integration processes in Azure and Talend (preferred).\n",
      "Act as internal subject matter expert on Azure and provide training and assistance to peers onshore/offshore.\n",
      "Analyze, Design, develop and deploy data integration pipelines, code and programs to massage and load data from various sources to multiple targets for internal BI use cases.\n",
      "Implement CICD for Azure pipelines, Mentoring internal teams on Azure best practices, provide assistance in appropriate services selection for business use cases.\n",
      "Manage system outages, maintain service availability, and provide production support for the platform and the solution hosted within it. Perform root cause analysis, resolve issues, initiate/execute process improvements, and implement bug fixes, patches, server maintenance, and configuration changes. Collaborate with stakeholders for activities ranging from gathering requirements to delivering solutions for problems/issues on data integration.\n",
      "Write test plans and test cases of the business process or application when needed. Document and share knowledge and findings with the team periodically.\n",
      "COVID-19 Vaccination\n",
      "Please be aware that Medline requires all employees starting in this position to be fully vaccinated against COVID-19. This position will require the successful candidate to provide proof that they are fully vaccinated by their start date. Medline is an equal opportunity employer, and will provide reasonable accommodations to those individuals who are unable to be vaccinated for COVID-19 consistent with federal, state, and local law.\n",
      "Qualifications:\n",
      "Bachelor’s Degree in Computer Science, Information Systems, or other related fields; or equivalent work experience.\n",
      "4+ years of experience with Cloud platforms and services on Azure.\n",
      "2+ years of strong ETL admin/development experience on all components/aspects of Talend preferred.\n",
      "Excellent communication skills, both written and verbal.\n",
      "Experience working in an offshore/onshore team model, experience working with diverse teams with various levels of skill sets.\n",
      "About Medline\n",
      "Medline Industries is the largest privately held manufacturer and distributor of healthcare supplies in the United States, providing more than 550,000 products that serve the entire continuum of care. Our innovative products and programs can be found in most hospitals, extended-care facilities, surgery centers, physician offices, home care dealers, home health agencies and retail outlets.\n",
      "Founded in 1910, Medline has grown from a small manufacturer of aprons, surgical gowns and uniforms to a thriving $17 billion global enterprise because of our dedicated people, entrepreneurial spirit and honest values.\n",
      "Again named one of the country’s \"Best and Brightest Companies to Work For” and once again named to Chicago Tribune’s Top Workplaces, Medline has experienced fifty-plus years of consecutive annual growth, and is headquartered in Northfield, IL.\n",
      "Primary Location\n",
      "US-IL-Northfield\n",
      "      \n",
      "        Medline Industries continues to grow, and grow, and grow. In fact, we've enjoyed DOUBLE DIGIT growth in 54 of the past 55 years\n",
      "Bachelor’s Degree in Computer Science, Information Systems, or other related fields; or equivalent work experience.\n",
      "4+ years\n",
      " of experience with Cloud platforms and services on Azure.\n",
      "2+ years\n",
      "Entry level\n",
      "Full-time\n",
      "Information Technology\n",
      "Medical Equipment Manufacturing\n",
      "2023-09-05 04:04:27\n",
      "https://www.linkedin.com/jobs/view/azure-data-engineer-administrator-at-medline-industries-lp-3610297973?trk=public_jobs_topcard-title\n",
      "3705364410\n",
      "Get It Recruit - Information Technology\n",
      "Chicago, IL\n",
      "Senior Associate Data Engineering - Remote | WFH\n",
      "\n",
      "        We are a leading digital transformation partner committed to helping organizations navigate their journey towards a digitally-enabled future. Our mission is to revolutionize the way businesses operate and serve their customers, combining innovative strategies, expert consulting, exceptional customer experiences, and cutting-edge engineering. We're a diverse and passionate team of over 20,000 individuals spread across 53 global offices, united by our core values and a shared purpose of driving excellence in every endeavor. Join us in our quest to empower people to thrive in the exciting pursuit of what's next.\n",
      "Job Description\n",
      "Position Overview:\n",
      "Are you ready to be a part of a dynamic team of technology experts? Publicis Sapient is seeking a Senior Data Engineer to join our ranks. In this role, you'll lead and deliver technical solutions for transformative digital projects on a grand scale. Working with the latest advancements in data technology, you'll play a pivotal role in guiding our clients toward a more digital future.\n",
      "Key Responsibilities\n",
      "Collaborate closely with clients to transform complex ideas into end-to-end solutions that drive their business forward.\n",
      "Lead the design, development, and delivery of large-scale data systems, data processing, and data transformation projects that deliver tangible business value.\n",
      "Automate data platform operations and manage post-production systems and processes.\n",
      "Conduct technical feasibility assessments and provide project estimates for solution design and development.\n",
      "Offer technical insights to agile processes, contributing to issue resolution and barrier removal throughout client engagements.\n",
      "Create and maintain infrastructure-as-code for cloud, on-premises, and hybrid environments using tools like Terraform, CloudFormation, Azure Resource Manager, Helm, and Google Cloud Deployment Manager.\n",
      "Mentor and support the growth of junior team members.\n",
      "Required Qualifications\n",
      "Proven experience with end-to-end data pipeline implementation in data platforms.\n",
      "Hands-on expertise with at least one leading public cloud data platform (Azure, AWS, or Google Cloud).\n",
      "Implementation experience with column-oriented and NoSQL database technologies, as well as traditional database systems.\n",
      "Experience in developing data pipelines for both streaming and batch integrations using tools/frameworks like Azure Data Factory, Glue ETL, Lambda, Spark, and Spark Streaming.\n",
      "Proficiency in data modeling, warehouse design, and fact/dimension implementations.\n",
      "Proficiency with code repositories and continuous integration.\n",
      "Skills in data processing programming (SQL, DBT, Python, etc.).\n",
      "Ability to work with various programming languages (Python, Spark, PySpark, Java, JavaScript, Scala).\n",
      "Expertise in data ingest, validation, and enrichment pipeline design.\n",
      "Strong understanding of cloud-native data platform design, with a focus on streaming and event-driven architectures.\n",
      "Experience with test programming, data validation, quality frameworks, and data lineage.\n",
      "Familiarity with metadata definition and management tools.\n",
      "Code review and mentorship experience.\n",
      "Bachelor's degree in Computer Science, Engineering, or a related field.\n",
      "Preferred Qualifications\n",
      "Developer certifications for cloud services (AWS, Google Cloud, Azure).\n",
      "Understanding of development and project methodologies.\n",
      "Willingness to travel.\n",
      "Remote work flexibility.\n",
      "Join our team and be part of the exciting future of digital transformation. Apply today and let's embark on this journey together.\n",
      "Salary: $ 46,000.00 130,000.00 Per Year\n",
      "      \n",
      "None\n",
      "Entry level\n",
      "Full-time\n",
      "Engineering and Information Technology\n",
      "Human Resources Services\n",
      "2023-09-06 04:04:28\n",
      "https://www.linkedin.com/jobs/view/senior-associate-data-engineering-remote-wfh-at-get-it-recruit-information-technology-3705364410?trk=public_jobs_topcard-title\n",
      "3708958224\n",
      "BDO USA\n",
      "Chicago, IL\n",
      "Tax Digital Transformation & Innovation Senior Data Engineer\n",
      "\n",
      "\n",
      "Job Description\n",
      "Job Summary:\n",
      "The Digital Transformation & Innovation (DT&I) Data Manager will work with the Data Engineering and Integration team to help develop and support BDO’s Tax Data Warehouse, manage and maintain data ingestion and Extract/Transform/Load processes, as well as provide support for the Power BI business intelligence platform and enterprise applications supporting the tax practice.\n",
      "The DT&I Data Manager will build and maintain tax data pipelines to support ad-hoc analytics and business intelligence applications as well as identify valuable development opportunities and ideas for improvement. This role will collaborate closely with data analytics teams to design, develop, and deploy new solutions that support strategic business priorities.\n",
      "Job Duties\n",
      " Develops software that processes, stores, and serves data for use by others \n",
      " Develops large scale data structures and pipelines to organize, collect, and standardize data to generate insights and address reporting needs \n",
      " Writes ETL processes, designs database systems, and develops tools for real time and offline analytic processing \n",
      " Develops and maintains optimal data pipelines into an advanced analytics platform, including design of data flows, procedures, and schedules. Ensures that optimal data pipelines are scalable, repeatable, and secure \n",
      " Troubleshoots software and processes for data consistency and integrity. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards \n",
      " Anticipates and prevents problems and roadblocks before they occur \n",
      " Interacts with internal Tax developers and external peers in Information Technology to exchange ideas and collaborate on data integration solutions \n",
      " Collaborates with data scientists to prepare data for model development \n",
      "Supervisory Responsibilities\n",
      " N/A \n",
      "Education\n",
      "Qualifications, Knowledge, Skills, and Abilities:\n",
      " Bachelor's degree and six (6) or more years of experience in the technology field, required; OR High School Diploma/GED and seven (7) or more years of experience in the technology field, required \n",
      " Bachelor’s degree with a focus in Information Systems, Computer Science, Engineering, Information Technology, or Mathematics, preferred \n",
      "Experience\n",
      " Tax or accounting experience, preferred \n",
      "License/Certifications\n",
      " Microsoft Azure Data Engineer, preferred \n",
      "Software\n",
      " Advanced experience with SQL, required \n",
      " Experience with Python, required \n",
      " Experience with Linux or Unix, required \n",
      " Data Definition Language (DDL), Data Manipulation Language (DML), views, functions, stored procedures, or performance tuning, preferred \n",
      " Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema Construction, preferred\n",
      " Experience with tax and/or accounting data, preferred \n",
      " Hands on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, preferred \n",
      " Experience with one (1) or more of the following computer languages, preferred:\n",
      " Java \n",
      " C#, C++ \n",
      " Scala \n",
      " Experience with tabular modeling within Power BI or Azure Analysis Services, preferred \n",
      " Experience with Git and DevOps deployment technologies, preferred \n",
      " Experience with one (1) or more of the following, preferred:\n",
      " AI Algorithms/Machine Learning \n",
      " Automation tools such as Data Factory, Data Bricks, Alteryx, etc. \n",
      " Computer Vision based AI technologies \n",
      "Language\n",
      " N/A \n",
      "Other Knowledge, Skills, & Abilities\n",
      " Demonstrated ability to work well remotely \n",
      " Solid verbal and written communication skills \n",
      " Strong interpersonal skills, including training/instruction with professionals at all levels \n",
      " Demonstrated sound decision-making skills \n",
      " Ability to work within teams \n",
      " Ability to complete projects independently \n",
      "About Us\n",
      "BDO delivers assurance, tax, digital technology solutions and financial advisory services to clients throughout the country and around the globe. We offer numerous industry-specific practices, world-class resources, and an unparalleled commitment to meeting our clients’ needs. We currently serve more than 400 publicly traded domestic and international clients.\n",
      "Unparalleled partner-involvement \n",
      "Deep industry knowledge and participation\n",
      "Geographic coverage across the U.S.\n",
      "Cohesive global network \n",
      "Focused capabilities across disciplines\n",
      "BDO brings world-class resources and exceptional service to each and every one of our clients. BDO USA is a member of BDO International, the world’s fifth largest accounting network.\n",
      "BDO offers a competitive Total Rewards package that encompass so much more than – “traditional benefits”. Our wide range of rewards and our employees’ ability to customize rewards to their individual needs are two of the reasons why BDO has been honored with so many workplace awards, including 100 Best Companies for Working Parents, Working Mother 100 Best Companies, Top Entry Level Employer, 2022 National Best & Brightest Companies to Work For and more.\n",
      "Some Examples Of Our Total Rewards Offerings Include\n",
      "Competitive pay and eligibility for an annual performance bonus. \n",
      "A 401k plan plus an employer match\n",
      "Comprehensive, medical, dental, vision, FSA, and prescription insurance from day one\n",
      " Competitive Paid Time Off with daily accrual from day one of employment, plus paid holidays \n",
      "Paid Parental Leave\n",
      "Adoption Assistance\n",
      "Firm paid life insurance\n",
      "Wellness programs\n",
      "Additional offerings include BDO Flex, Group Legal insurance, Pet insurance and Long-Term Care Insurance \n",
      "Above offerings may be subject to eligibility requirements.\n",
      "Click here to find out more!\n",
      "All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status.\n",
      "\"BDO USA, P.A. is an EO employer M/F/Veteran/Disability\"\n",
      "\n",
      "\n",
      " Bachelor's degree and six (6) or more years of experience in the technology field, required; OR High School Diploma/GED and seven (7) or more years\n",
      "Entry level\n",
      "Full-time\n",
      "Finance and Accounting/Auditing\n",
      "Accounting\n",
      "2023-09-09 04:04:28\n",
      "https://www.linkedin.com/jobs/view/tax-digital-transformation-innovation-senior-data-engineer-at-bdo-usa-3708958224?trk=public_jobs_topcard-title\n",
      "3708954564\n",
      "BDO USA\n",
      "Rosemont, IL\n",
      "Tax Digital Transformation & Innovation Senior Data Engineer\n",
      "\n",
      "\n",
      "Job Description\n",
      "Job Summary:\n",
      "The Digital Transformation & Innovation (DT&I) Data Manager will work with the Data Engineering and Integration team to help develop and support BDO’s Tax Data Warehouse, manage and maintain data ingestion and Extract/Transform/Load processes, as well as provide support for the Power BI business intelligence platform and enterprise applications supporting the tax practice.\n",
      "The DT&I Data Manager will build and maintain tax data pipelines to support ad-hoc analytics and business intelligence applications as well as identify valuable development opportunities and ideas for improvement. This role will collaborate closely with data analytics teams to design, develop, and deploy new solutions that support strategic business priorities.\n",
      "Job Duties\n",
      " Develops software that processes, stores, and serves data for use by others \n",
      " Develops large scale data structures and pipelines to organize, collect, and standardize data to generate insights and address reporting needs \n",
      " Writes ETL processes, designs database systems, and develops tools for real time and offline analytic processing \n",
      " Develops and maintains optimal data pipelines into an advanced analytics platform, including design of data flows, procedures, and schedules. Ensures that optimal data pipelines are scalable, repeatable, and secure \n",
      " Troubleshoots software and processes for data consistency and integrity. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards \n",
      " Anticipates and prevents problems and roadblocks before they occur \n",
      " Interacts with internal Tax developers and external peers in Information Technology to exchange ideas and collaborate on data integration solutions \n",
      " Collaborates with data scientists to prepare data for model development \n",
      "Supervisory Responsibilities\n",
      " N/A \n",
      "Education\n",
      "Qualifications, Knowledge, Skills, and Abilities:\n",
      " Bachelor's degree and six (6) or more years of experience in the technology field, required; OR High School Diploma/GED and seven (7) or more years of experience in the technology field, required \n",
      " Bachelor’s degree with a focus in Information Systems, Computer Science, Engineering, Information Technology, or Mathematics, preferred \n",
      "Experience\n",
      " Tax or accounting experience, preferred \n",
      "License/Certifications\n",
      " Microsoft Azure Data Engineer, preferred \n",
      "Software\n",
      " Advanced experience with SQL, required \n",
      " Experience with Python, required \n",
      " Experience with Linux or Unix, required \n",
      " Data Definition Language (DDL), Data Manipulation Language (DML), views, functions, stored procedures, or performance tuning, preferred \n",
      " Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema Construction, preferred\n",
      " Experience with tax and/or accounting data, preferred \n",
      " Hands on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, preferred \n",
      " Experience with one (1) or more of the following computer languages, preferred:\n",
      " Java \n",
      " C#, C++ \n",
      " Scala \n",
      " Experience with tabular modeling within Power BI or Azure Analysis Services, preferred \n",
      " Experience with Git and DevOps deployment technologies, preferred \n",
      " Experience with one (1) or more of the following, preferred:\n",
      " AI Algorithms/Machine Learning \n",
      " Automation tools such as Data Factory, Data Bricks, Alteryx, etc. \n",
      " Computer Vision based AI technologies \n",
      "Language\n",
      " N/A \n",
      "Other Knowledge, Skills, & Abilities\n",
      " Demonstrated ability to work well remotely \n",
      " Solid verbal and written communication skills \n",
      " Strong interpersonal skills, including training/instruction with professionals at all levels \n",
      " Demonstrated sound decision-making skills \n",
      " Ability to work within teams \n",
      " Ability to complete projects independently \n",
      "About Us\n",
      "BDO delivers assurance, tax, digital technology solutions and financial advisory services to clients throughout the country and around the globe. We offer numerous industry-specific practices, world-class resources, and an unparalleled commitment to meeting our clients’ needs. We currently serve more than 400 publicly traded domestic and international clients.\n",
      "Unparalleled partner-involvement \n",
      "Deep industry knowledge and participation\n",
      "Geographic coverage across the U.S.\n",
      "Cohesive global network \n",
      "Focused capabilities across disciplines\n",
      "BDO brings world-class resources and exceptional service to each and every one of our clients. BDO USA is a member of BDO International, the world’s fifth largest accounting network.\n",
      "BDO offers a competitive Total Rewards package that encompass so much more than – “traditional benefits”. Our wide range of rewards and our employees’ ability to customize rewards to their individual needs are two of the reasons why BDO has been honored with so many workplace awards, including 100 Best Companies for Working Parents, Working Mother 100 Best Companies, Top Entry Level Employer, 2022 National Best & Brightest Companies to Work For and more.\n",
      "Some Examples Of Our Total Rewards Offerings Include\n",
      "Competitive pay and eligibility for an annual performance bonus. \n",
      "A 401k plan plus an employer match\n",
      "Comprehensive, medical, dental, vision, FSA, and prescription insurance from day one\n",
      " Competitive Paid Time Off with daily accrual from day one of employment, plus paid holidays \n",
      "Paid Parental Leave\n",
      "Adoption Assistance\n",
      "Firm paid life insurance\n",
      "Wellness programs\n",
      "Additional offerings include BDO Flex, Group Legal insurance, Pet insurance and Long-Term Care Insurance \n",
      "Above offerings may be subject to eligibility requirements.\n",
      "Click here to find out more!\n",
      "All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status.\n",
      "\"BDO USA, P.A. is an EO employer M/F/Veteran/Disability\"\n",
      "\n",
      "\n",
      " Bachelor's degree and six (6) or more years of experience in the technology field, required; OR High School Diploma/GED and seven (7) or more years\n",
      "Entry level\n",
      "Full-time\n",
      "Finance and Accounting/Auditing\n",
      "Accounting\n",
      "2023-09-09 04:04:28\n",
      "https://www.linkedin.com/jobs/view/tax-digital-transformation-innovation-senior-data-engineer-at-bdo-usa-3708954564?trk=public_jobs_topcard-title\n",
      "3708953838\n",
      "BDO USA\n",
      "Oak Brook, IL\n",
      "Tax Digital Transformation & Innovation Senior Data Engineer\n",
      "\n",
      "\n",
      "Job Description\n",
      "Job Summary:\n",
      "The Digital Transformation & Innovation (DT&I) Data Manager will work with the Data Engineering and Integration team to help develop and support BDO’s Tax Data Warehouse, manage and maintain data ingestion and Extract/Transform/Load processes, as well as provide support for the Power BI business intelligence platform and enterprise applications supporting the tax practice.\n",
      "The DT&I Data Manager will build and maintain tax data pipelines to support ad-hoc analytics and business intelligence applications as well as identify valuable development opportunities and ideas for improvement. This role will collaborate closely with data analytics teams to design, develop, and deploy new solutions that support strategic business priorities.\n",
      "Job Duties\n",
      " Develops software that processes, stores, and serves data for use by others \n",
      " Develops large scale data structures and pipelines to organize, collect, and standardize data to generate insights and address reporting needs \n",
      " Writes ETL processes, designs database systems, and develops tools for real time and offline analytic processing \n",
      " Develops and maintains optimal data pipelines into an advanced analytics platform, including design of data flows, procedures, and schedules. Ensures that optimal data pipelines are scalable, repeatable, and secure \n",
      " Troubleshoots software and processes for data consistency and integrity. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards \n",
      " Anticipates and prevents problems and roadblocks before they occur \n",
      " Interacts with internal Tax developers and external peers in Information Technology to exchange ideas and collaborate on data integration solutions \n",
      " Collaborates with data scientists to prepare data for model development \n",
      "Supervisory Responsibilities\n",
      " N/A \n",
      "Education\n",
      "Qualifications, Knowledge, Skills, and Abilities:\n",
      " Bachelor's degree and six (6) or more years of experience in the technology field, required; OR High School Diploma/GED and seven (7) or more years of experience in the technology field, required \n",
      " Bachelor’s degree with a focus in Information Systems, Computer Science, Engineering, Information Technology, or Mathematics, preferred \n",
      "Experience\n",
      " Tax or accounting experience, preferred \n",
      "License/Certifications\n",
      " Microsoft Azure Data Engineer, preferred \n",
      "Software\n",
      " Advanced experience with SQL, required \n",
      " Experience with Python, required \n",
      " Experience with Linux or Unix, required \n",
      " Data Definition Language (DDL), Data Manipulation Language (DML), views, functions, stored procedures, or performance tuning, preferred \n",
      " Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema Construction, preferred\n",
      " Experience with tax and/or accounting data, preferred \n",
      " Hands on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, preferred \n",
      " Experience with one (1) or more of the following computer languages, preferred:\n",
      " Java \n",
      " C#, C++ \n",
      " Scala \n",
      " Experience with tabular modeling within Power BI or Azure Analysis Services, preferred \n",
      " Experience with Git and DevOps deployment technologies, preferred \n",
      " Experience with one (1) or more of the following, preferred:\n",
      " AI Algorithms/Machine Learning \n",
      " Automation tools such as Data Factory, Data Bricks, Alteryx, etc. \n",
      " Computer Vision based AI technologies \n",
      "Language\n",
      " N/A \n",
      "Other Knowledge, Skills, & Abilities\n",
      " Demonstrated ability to work well remotely \n",
      " Solid verbal and written communication skills \n",
      " Strong interpersonal skills, including training/instruction with professionals at all levels \n",
      " Demonstrated sound decision-making skills \n",
      " Ability to work within teams \n",
      " Ability to complete projects independently \n",
      "About Us\n",
      "BDO delivers assurance, tax, digital technology solutions and financial advisory services to clients throughout the country and around the globe. We offer numerous industry-specific practices, world-class resources, and an unparalleled commitment to meeting our clients’ needs. We currently serve more than 400 publicly traded domestic and international clients.\n",
      "Unparalleled partner-involvement \n",
      "Deep industry knowledge and participation\n",
      "Geographic coverage across the U.S.\n",
      "Cohesive global network \n",
      "Focused capabilities across disciplines\n",
      "BDO brings world-class resources and exceptional service to each and every one of our clients. BDO USA is a member of BDO International, the world’s fifth largest accounting network.\n",
      "BDO offers a competitive Total Rewards package that encompass so much more than – “traditional benefits”. Our wide range of rewards and our employees’ ability to customize rewards to their individual needs are two of the reasons why BDO has been honored with so many workplace awards, including 100 Best Companies for Working Parents, Working Mother 100 Best Companies, Top Entry Level Employer, 2022 National Best & Brightest Companies to Work For and more.\n",
      "Some Examples Of Our Total Rewards Offerings Include\n",
      "Competitive pay and eligibility for an annual performance bonus. \n",
      "A 401k plan plus an employer match\n",
      "Comprehensive, medical, dental, vision, FSA, and prescription insurance from day one\n",
      " Competitive Paid Time Off with daily accrual from day one of employment, plus paid holidays \n",
      "Paid Parental Leave\n",
      "Adoption Assistance\n",
      "Firm paid life insurance\n",
      "Wellness programs\n",
      "Additional offerings include BDO Flex, Group Legal insurance, Pet insurance and Long-Term Care Insurance \n",
      "Above offerings may be subject to eligibility requirements.\n",
      "Click here to find out more!\n",
      "All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status.\n",
      "\"BDO USA, P.A. is an EO employer M/F/Veteran/Disability\"\n",
      "\n",
      "\n",
      " Bachelor's degree and six (6) or more years of experience in the technology field, required; OR High School Diploma/GED and seven (7) or more years\n",
      "Entry level\n",
      "Full-time\n",
      "Finance and Accounting/Auditing\n",
      "Accounting\n",
      "2023-09-09 04:04:29\n",
      "https://www.linkedin.com/jobs/view/tax-digital-transformation-innovation-senior-data-engineer-at-bdo-usa-3708953838?trk=public_jobs_topcard-title\n",
      "3714774167\n",
      "LinkedIn\n",
      "San Francisco Bay Area\n",
      "Research Data Engineer\n",
      "\n",
      "        LinkedIn is the world’s largest professional network, built to create economic opportunity for every member of the global workforce. Our products help people make powerful connections, discover exciting opportunities, build necessary skills, and gain valuable insights every day. We’re also committed to providing transformational opportunities for our own employees by investing in their growth. We aspire to create a culture that’s built on trust, care, inclusion, and fun – where everyone can succeed.\n",
      "Join us to transform the way the world works.\n",
      "At LinkedIn, we trust each other to do our best work where it works best for us and our teams. This role offers a hybrid work option, meaning you can both work from home and commute to the SF or Sunnyvale LinkedIn office, depending on what’s best for you and when it is important for your team to be together.\n",
      "As a Research Data Engineer supporting LinkedIn Market Research you will leverage one of the richest proprietary data sets in the world to help standardize and scale research programs while answering challenging problems: How do we best advise our internal clients about Market Research? How do we scale and standardize our current tools, programs, and processes to support a growing demand? How do we measure the value and performance of our programs and tools? You will help the team leverage survey results to influence decision making and democratize member insights through innovative data pipelines, structures, programs, and processes.\n",
      "Responsibilities\n",
      "Leverage LinkedIn’s technology stack to standardize and scale Research Programs to support cross-functional partners\n",
      "Create requirements and project plans for research tools, platforms, and processes and prioritize based on impact and effort to complete\n",
      "Proactively and effectively communicate timelines and deadlines, challenges and milestones with research stakeholders and managers\n",
      "Deliver data ETLs, tools/platforms and reporting in a timely and rigorous manner\n",
      "Work closely with our Market Research Managers and other Program Managers to identify opportunities to conceptualize, develop, implement, and scale effective and sustainable research tools, platforms, and programs\n",
      " Leverage internal Engineering tools and teams in order to shape our tech strategy and build cross-functional momentum for operational improvements\n",
      "Explore, extract, analyze and craft data-driven narratives through compelling visualizations in Tableau and internal front-end systems.\n",
      "Evangelize programs and tools to internal team and key stakeholders\n",
      "Basic Qualifications\n",
      "BA/BS degree or equivalent experience \n",
      "3+ years of experience with managing big data pipelines, data structuring and foundations, data analysis, data storytelling, turning data into insights and presenting to internal and/or external stakeholders\n",
      "3+ years of experience with project or program management and working cross-functionally\n",
      "2+ years of experience with SQL and Tableau or equivalent\n",
      "Experience with Spark/Hive/Python/R/JavaScript (Hadoop)\n",
      "Experience with version control systems (e.g., Git) and code deployment best practices\n",
      "Preferred Qualifications\n",
      "Strong business acumen and creative problem-solving skills\n",
      "Strong intellectual curiosity; desire and ability to learn\n",
      "Experience managing complex, multi-workstream programs\n",
      "Ability to communicate findings clearly to both technical and non-technical audiences\n",
      "Strong stakeholder management skills and the ability to influence peers and managers\n",
      "Pro-activeness and creativity in troubleshooting and problem-solving\n",
      "Strong attention to detail \n",
      "Thorough QC of your own work before delivery\n",
      "Experience/Familiarity with 3rd party data ingestion and regression pipelines\n",
      "Experience or strong interest in Generative AI application development (LLMs like GPT-4, Langchain, etc.)\n",
      "Experience leveraging data to drive research impact\n",
      "Suggested Skills\n",
      "SQL \n",
      "Tableau \n",
      "Experience with Version Control systems \n",
      "Project Management \n",
      "Building and maintaining data pipelines\n",
      "LinkedIn is committed to fair and equitable compensation practices.\n",
      "The pay range for this role is $93,000 to $145,000. Actual compensation packages are based on several factors that are unique to each candidate, including but not limited to skill set, depth of experience, certifications, and specific work location. This may be different in other locations due to differences in the cost of labor.\n",
      "The total compensation package for this position may also include annual performance bonus, stock, benefits and/or other applicable incentive compensation plans. For more information, visit https://careers.linkedin.com/benefits.\n",
      "Equal Opportunity Statement\n",
      "LinkedIn is committed to diversity in its workforce and is proud to be an equal opportunity employer. LinkedIn considers qualified applicants without regard to race, color, religion, creed, gender, national origin, age, disability, veteran status, marital status, pregnancy, sex, gender expression or identity, sexual orientation, citizenship, or any other legally protected class. LinkedIn is an Affirmative Action and Equal Opportunity Employer as described in our equal opportunity statement here: https://lnkd.in/equalemploymentopportunity2017. Please reference the following information for more information: https://legal.linkedin.com/content/dam/legal/LinkedIn_EEO_Statement_2020.pdf. Please reference the following information for more information: https://www.eeoc.gov/sites/default/files/migrated_files/employers/poster_screen_reader_optimized.pdf and https://www.dol.gov/ofccp/regs/compliance/posters/pdf/OFCCP_EEO_Supplement_Final_JRF_QA_508c.pdf for more information.\n",
      "LinkedIn is committed to offering an inclusive and accessible experience for all job seekers, including individuals with disabilities. Our goal is to foster an inclusive and accessible workplace where everyone has the opportunity to be successful.\n",
      "If you need a reasonable accommodation to search for a job opening, apply for a position, or participate in the interview process, connect with us at accommodations@linkedin.com and describe the specific accommodation requested for a disability-related limitation.\n",
      "Examples Of Reasonable Accommodations Include But Are Not Limited To\n",
      "Reasonable accommodations are modifications or adjustments to the application or hiring process that would enable you to fully participate in that process.\n",
      "Documents in alternate formats or read aloud to you\n",
      "Having interviews in an accessible location\n",
      "Being accompanied by a service dog\n",
      "Having a sign language interpreter present for the interview\n",
      "A request for an accommodation will be responded to within three business days. However, non-disability related requests, such as following up on an application, will not receive a response.\n",
      "LinkedIn will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by LinkedIn, or (c) consistent with LinkedIn's legal duty to furnish information.\n",
      "San Francisco Fair Chance Ordinance \n",
      "Pursuant to the San Francisco Fair Chance Ordinance, LinkedIn will consider for employment qualified applicants with arrest and conviction records.\n",
      "Pay Transparency Policy Statement \n",
      "As a federal contractor, LinkedIn follows the Pay Transparency and non-discrimination provisions described at this link: https://lnkd.in/paytransparency.\n",
      "Global Data Privacy Notice for Job Candidates \n",
      "Please follow this link to access the document that provides transparency around the way in which LinkedIn handles personal data of employees and job applicants: https://legal.linkedin.com/candidate-portal.\n",
      "      \n",
      "BA/BS degree or equivalent experience \n",
      "3+ years\n",
      " of experience with managing big data pipelines, data structuring and foundations, data analysis, data storytelling, turning data into insights and presenting to internal and/or external stakeholders\n",
      "3+ years\n",
      " of experience with project or program management and working cross-functionally\n",
      "2+ years\n",
      "Entry level\n",
      "Full-time\n",
      "Information Technology\n",
      "IT Services and IT Consulting\n",
      "2023-09-08 04:39:36\n",
      "https://www.linkedin.com/jobs/view/research-data-engineer-at-linkedin-3714774167?trk=public_jobs_topcard-title\n",
      "3708157029\n",
      "RIT Solutions, Inc.\n",
      "Sunnyvale, CA\n",
      "Azure SQL Developer/Data Engineer\n",
      "\n",
      "\n",
      "Title: Azure SQL Developer/Data Engineer\n",
      "Duration: 12 months with possible extensions\n",
      "About The Role\n",
      " Data Bricks experience required\n",
      " ADF experience required\n",
      " Azure Data Warehouse experience required.\n",
      " Azure Cloud experience required.\n",
      " Managed healthcare experience required.\n",
      "Essential Functions\n",
      " Primary function: Uses analytical, data and SQL development skills to\n",
      "produce reports to support the Business in making data driven business decisions.\n",
      " Includes producing both ad-hoc and regularly scheduled reports.\n",
      " Reviews data to ensure validity.\n",
      " Cloud - Azure experience required, Dev-Ops and CI/CD experience required.\n",
      "Experience\n",
      "SKILLS & ABILITIES\n",
      " 1 plus years of experience in hands-on with SSIS, SSRS, SSAS design.\n",
      " 5 plus years of experience in SQL development-developing and designing\n",
      " applications and/or reports on SQL server 2014.\n",
      " DevOps and CI/CD experience\n",
      " Native Azure SQL Development required\n",
      " Experience designing and developing applications on SQL Server 2014\n",
      " Hands-on experience in writing T-SQL queries, Stored Procedures, User Defined\n",
      " Functions, Jobs, Cursors, Views and Triggers, optimized for database efficiency.\n",
      " Hands-on experience with SSIS, SSRS, SSAS design and development\n",
      "\n",
      "\n",
      " 1 plus years\n",
      " 5 plus years\n",
      "Entry level\n",
      "Contract\n",
      "Information Technology\n",
      "Staffing and Recruiting\n",
      "2023-09-08 04:39:37\n",
      "https://www.linkedin.com/jobs/view/azure-sql-developer-data-engineer-at-rit-solutions-inc-3708157029?trk=public_jobs_topcard-title\n",
      "3715560870\n",
      "TechFetch.com - On Demand Tech Workforce hiring platform\n",
      "San Jose, CA\n",
      "Remote - Azure Data Engineer with Data Bricks\n",
      "\n",
      "        \"ALL our jobs are US based and candidates must be in the US with valid US Work Authorization. Please apply on our website directly.\" ob role: Azure Data Engineer with Data BricksOpenings : 10+Location: All over USA it will be Remote but for Dallas location it is Onsite (Hybrid)Note: They have openings for L2/L3/L4/L5.Bill rate will vary depending on exp.Required Skills/Qualifications: 6+ years of relevant experience\n",
      "Bachelor's and/or master s degree in computer science or equivalent experience.\n",
      "Strong communication, analytical and problem-solving skills with a high attention to detail. Desired Experience: 10+ years of experience building and leading highly complex, technical engineering teams.\n",
      "Strong hands-on experience in Data bricks\n",
      "Implement scalable and sustainable data engineering solutions using tools such as Data bricks, Azure, Apache Spark, and Python. The data pipelines must be created, maintained, and optimized as workloads move from development to production for specific use cases.\n",
      "Experience Managing Distributed Teams Preferred.\n",
      "Comfortable working with ambiguity and multiple stakeholders.\n",
      "Comfortable working cross functionality with product management and directly with customers; ability to deeply understand product and customer personas.\n",
      "Expertise on Azure Cloud platform\n",
      "Good SQL knowledge\n",
      "Knowledge on orchestrating workloads on cloud\n",
      "Ability to set and lead the technical vision while balancing business drivers\n",
      "Strong experience with PySpark, Python programming\n",
      "Proficiency with APIs, containerization and orchestration is a plus. Experience handling large and complex sets of data from various sources and databases. Solid grasp of database engineering and design principles\n",
      "Familiarity with CI/CD methods desired. Good to have Teradata Experience (not MandatoryQualifications: Bachelor's and/or master s degreeAbout you: You are self-motivated, collaborative, eager to learn, and hands on You love trying out new apps, and find yourself coming up with ideas to improve them You stay ahead with all the latest trends and technologies You are particular about following industry best practices and have high standards regarding quality\n",
      "      \n",
      "        \"ALL our jobs are US based and candidates must be in the US with valid US Work Authorization. Please apply on our website directly.\" ob role: Azure Data Engineer with Data BricksOpenings : 10+Location: All over USA it will be Remote but for Dallas location it is Onsite (Hybrid)Note: They have openings for L2/L3/L4/L5.Bill rate will vary depending on exp.Required Skills/Qualifications: 6+ years\n",
      "Strong communication, analytical and problem-solving skills with a high attention to detail. Desired Experience: 10+ years\n",
      "Entry level\n",
      "Part-time\n",
      "Information Technology\n",
      "IT Services and IT Consulting\n",
      "2023-09-09 04:39:38\n",
      "https://www.linkedin.com/jobs/view/remote-azure-data-engineer-with-data-bricks-at-techfetch-com-on-demand-tech-workforce-hiring-platform-3715560870?trk=public_jobs_topcard-title\n",
      "3708150943\n",
      "Tanisha Systems, Inc\n",
      "San Jose, CA\n",
      "Azure Data Engineer with Data Bricks\n",
      "\n",
      "        ob role: \n",
      "Azure Data Engineer with Data Bricks\n",
      "Openings : 10+\n",
      "Location: All over USA it will be Remote but for Dallas location it is Onsite (Hybrid)\n",
      "Note: They have openings for L2/L3/L4/L5.Bill rate will vary depending on exp.\n",
      "Required Skills/Qualifications\n",
      "6+ years of relevant experience \n",
      "Bachelor's and/or masters degree in computer science or equivalent experience. \n",
      "Strong communication, analytical and problem-solving skills with a high attention to detail. \n",
      " \n",
      "Desired Experience:\n",
      "10+ years of experience building and leading highly complex, technical engineering teams. \n",
      "Strong hands-on experience in Data bricks \n",
      "Implement scalable and sustainable data engineering solutions using tools such as Data bricks, Azure, Apache Spark, and Python. The data pipelines must be created, maintained, and optimized as workloads move from development to production for specific use cases. \n",
      "Experience managing distributed teams preferred. \n",
      "Comfortable working with ambiguity and multiple stakeholders. \n",
      "Comfortable working cross functionality with product management and directly with customers; ability to deeply understand product and customer personas. \n",
      "Expertise on Azure Cloud platform \n",
      "Good SQL knowledge \n",
      "Knowledge on orchestrating workloads on cloud \n",
      "Ability to set and lead the technical vision while balancing business drivers \n",
      "Strong experience with PySpark, Python programming \n",
      "Proficiency with APIs, containerization and orchestration is a plus.\n",
      "Experience handling large and complex sets of data from various sources and databases.\n",
      "Solid grasp of database engineering and design principles \n",
      "Familiarity with CI/CD methods desired.\n",
      "Good to have Teradata Experience (not Mandatory).\n",
      " \n",
      "Qualifications:\n",
      "Bachelor's and/or masters degree\n",
      " \n",
      "About you:\n",
      "You are self-motivated, collaborative, eager to learn, and hands on\n",
      "You love trying out new apps, and find yourself coming up with ideas to improve them\n",
      "You stay ahead with all the latest trends and technologies\n",
      "You are particular about following industry best practices and have high standards regarding quality\n",
      "\n",
      "\n",
      "Required Skills/Qualifications\n",
      "6+ years\n",
      "Desired Experience:\n",
      "10+ years\n",
      "Entry level\n",
      "Contract\n",
      "Information Technology\n",
      "Human Resources Services\n",
      "2023-09-08 04:39:38\n",
      "https://www.linkedin.com/jobs/view/azure-data-engineer-with-data-bricks-at-tanisha-systems-inc-3708150943?trk=public_jobs_topcard-title\n",
      "3712851766\n",
      "SPECTRAFORCE\n",
      "Cupertino, CA\n",
      "Video Data Collector\n",
      "\n",
      "\n",
      "Job Title: Video Data Collector\n",
      "Duration: 6 months\n",
      "Location: Cupertino, CA 95014P\n",
      "R: $29.88/hr\n",
      "Job Summary\n",
      "The Video Engineering group is looking for a Video Data Collector. The candidate for this position will collaborate with a data engineer, HW/SW research engineers, and project managers to develop and execute small-scale data-gathering efforts. We work on multiple, related projects requiring creativity and resourcefulness. You will be able to demonstrate your skills and expertise as we work together to innovate. We are an unusual team in how we operate, and we are looking for creative individuals and anyone with room to grow to join us. We are unusual in the sense that our work fills in a gap between Algorithm teams, Quality Assurance (QA), and User Studies. Do you think differently? Are you passionate about meaningful innovation? We often think, tinker, debate, and investigate fun problems with a high level of transparency. We also tackle a diversity of problems; as a result, work is entertaining, captivating, and exciting.\n",
      "Responsibilities\n",
      "Follow the protocols to collect the video data by using DSLR and iPhone\n",
      "Work closely with engineers to support process work\n",
      "Ability to organize and execute complex, detailed capture procedures/sceneries setup\n",
      "Offload the data from devices and upload to the internal platform by using internal tools\n",
      "Maintain a clean, orderly, and safe work environment and documentation\n",
      "Key Qualifications\n",
      "Must be able to work independently as well with other team members\n",
      "Able to follow standard operating procedures and safety protocols\n",
      "Self-starter with time management capability and a can-do attitude\n",
      "Excellent verbal and written communication skills\n",
      "Experience with iOS, Mac OS, and other productivity tools\n",
      "Experience with DSLR concept and usage\n",
      "Education\n",
      "Bachelor's degree required\n",
      " \n",
      "Language Skills\n",
      "English required\n",
      "\n",
      "\n",
      "None\n",
      "Entry level\n",
      "Contract\n",
      "Art/Creative and Writing/Editing\n",
      "Staffing and Recruiting\n",
      "2023-09-09 04:39:39\n",
      "https://www.linkedin.com/jobs/view/video-data-collector-at-spectraforce-3712851766?trk=public_jobs_topcard-title\n",
      "3707550926\n",
      "TalentBurst, an Inc 5000 company\n",
      "Cupertino, CA\n",
      "Video Data Collector #: 23-15206\n",
      "\n",
      "\n",
      "Job Description\n",
      " Title: Video Data Collector\n",
      " Location: Cupertino, CA 95014 \n",
      " Duration: 6 Months &plus; \n",
      " 25198334\n",
      " 100 % ONSITE \n",
      "Job Summary\n",
      "The Video Engineering group is looking for a Video Data Collector. The candidate for this position will collaborate with data engineer, HW/SW research engineers, and project managers to develop and execute small-scale data gathering efforts. We work on multiple, related projects requiring creativity and resourcefulness. You will be able to demonstrate your skills and expertise as we work together to innovate. We are an unusual team in how we operate, and we are looking for creative individual and anyone with room to grow to join us. We are unusual in the sense that our work fills in a gap between Algorithm teams, Quality Assurance (QA), and User Studies. Do you think differently? Are you passionate about meaningful innovation? We often think, tinker, debate, and investigate fun problems with a high level of transparency. We also tackle a diversity of problems; as a result, work is entertaining, captivating, and exciting.\n",
      "Responsibilities Include.\n",
      " Follow the protocols to collect the video data by using DSLR and iPhone.\n",
      " Work closely with engineers to support process work.\n",
      " Ability to organize and execute complex, detailed capture procedures/sceneries setup.\n",
      " Offload the data from devices and upload to internal platform by using internal tools.\n",
      " Maintain a clean, orderly, and safe work environment and documentation.\n",
      "Key Qualifications\n",
      " Must be able to work independently as well with other team members.\n",
      " Able to follow standard operating procedures and safety protocols.\n",
      " Self-starter with time management capability and a can do attitude.\n",
      " Excellent verbal and written communication skills\n",
      " Experience with iOS, Mac OS and other productivity tools\n",
      " Experience with DSLR concept and usage\n",
      " Education\n",
      " Bachelors degree required\n",
      "Language Skills\n",
      " English required.\n",
      " \n",
      " Required Skills:\n",
      "MACOS\n",
      "PROCESS TECHNICIAN\n",
      "TIME MANAGEMENT\n",
      "SELF-STARTER\n",
      "Additional Skills\n",
      "EXCELLENT VERBAL AND WRITTEN COMMUNICATION SKILLS\n",
      "HIGH ACCURACY\n",
      "PROCESS DEVELOPMENT\n",
      " TB_HL \n",
      "Job #: 23-15206\n",
      "\n",
      "\n",
      "None\n",
      "Entry level\n",
      "Contract\n",
      "Accounting/Auditing and Finance\n",
      "Staffing and Recruiting\n",
      "2023-09-09 04:39:39\n",
      "https://www.linkedin.com/jobs/view/video-data-collector-%23-23-15206-at-talentburst-an-inc-5000-company-3707550926?trk=public_jobs_topcard-title\n"
     ]
    }
   ],
   "source": [
    "cs = '''(description= (retry_count=20)(retry_delay=3)(address=(protocol=tcps)(port=1521)(host=adb.us-sanjose-1.oraclecloud.com))(connect_data=(service_name=ga3e236c6957ba6_oltpdb_high.adb.oraclecloud.com))(security=(ssl_server_dn_match=yes)))'''\n",
    "user = \"appuser\"\n",
    "password = os.environ['ORACLE_PASSWORD_APPUSER']\n",
    "engine = create_engine(\n",
    "    f'oracle+oracledb://{user}:{password}@{cs}'\n",
    ")\n",
    "\n",
    "\n",
    "with engine.begin() as conn:\n",
    "        df_results = pd.read_sql_query(f\"SELECT * FROM tbl_jobs\", conn)\n",
    "        #results = conn.execute(text(f\"SELECT * FROM tbl_jobs WHERE job_title like '%{title}%'\"))\n",
    "        #results = conn.execute(text(f\"SELECT * FROM tbl_jobs\"))\n",
    "        results = df_results.to_dict(\"records\")\n",
    "\n",
    "\n",
    "#jobs = db.query_db(title, location)\n",
    "\n",
    "for job in results:\n",
    "        print(job[\"job_id\"])\n",
    "        print(job[\"company\"])\n",
    "        print(job[\"location\"])\n",
    "        print(job[\"job_title\"])\n",
    "        print(job[\"job_description\"])\n",
    "        print(job[\"experience\"])\n",
    "        print(job[\"seniority_level\"])\n",
    "        print(job[\"employment_type\"])\n",
    "        print(job[\"job_function\"])\n",
    "        print(job[\"industries\"])\n",
    "        print(job[\"posting_date\"])\n",
    "        print(job[\"url\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'San Jose, California, United States'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as conn:\n",
    "    tracking_all_job_ids = pd.read_sql_query(f'SELECT job_id FROM tbl_jobs', conn)\n",
    "    tracking_all_job_ids = tracking_all_job_ids.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3610297973],\n",
       "       [3674224518],\n",
       "       [3676035101],\n",
       "       [3705364410],\n",
       "       [3707507616],\n",
       "       [3707532523],\n",
       "       [3707550926],\n",
       "       [3708148376],\n",
       "       [3708150943],\n",
       "       [3708157029],\n",
       "       [3708952912],\n",
       "       [3708953838],\n",
       "       [3708954564],\n",
       "       [3708958224],\n",
       "       [3708984293],\n",
       "       [3712609923],\n",
       "       [3712851766],\n",
       "       [3714572531],\n",
       "       [3714774167],\n",
       "       [3715560870]], dtype=int64)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracking_all_job_ids.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3708985762, 3708985763, 3708985764, ..., 3718985759, 3718985760,\n",
       "       3718985761], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = 3708985762\n",
    "np_test = np.arange(start, start+10000000) # 10 million\n",
    "np_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3708952912, 3714572531], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3714572531 in np_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n",
      "30.517578125 MB\n",
      "10000000\n",
      "76.29405212402344 MB\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(len(test_range))\n",
    "print(sys.getsizeof(test_range[1])*len(test_range)/1024/1024, \"MB\")\n",
    "\n",
    "print(len(np_test))\n",
    "print(sys.getsizeof(np_test)/1024/1024, \"MB\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
